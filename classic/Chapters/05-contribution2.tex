\chapter{Prediction}\label{chap:contrib2}

In this chapter we will introduce our model for providing predictions based on the collected dataset. We will go into the idea and decisions in creating it in this specific way and provide insight into the implementation.


\section{Model Selection}
Since we are interested in time and energy, we need two prediction models. More precisely, we need one for runtime predictions and one for power predictions. Multiplying the two predictions we obtain the our predicted energy. \\
In order to retain as much understanding of the prediction process as we can while ensuring tolerable execution times for the prediction times, we do not want to use a full DNN to perform the predictions. Instead we are looking for a more lightweight solution, closer to classical statistics. These requirements led us to using a random forest predictor model. It is both lightweight and sets restrictions to its input which force us to format our dataset in a way that provides some insight into cause and effect for the predictions. \\
From the same family of tree based ensemble methods, we also ran some tests with Extremely Randomized Trees, but the increased randomness did not yield better results. We therefore continued with the less complex random forest. A similar attempt was made with Extreme Gradient Boosting, but this also did not provide and increase in prediction accuracy justifying the increased training time. While we did not observe any reason to use on of these methods, we do not want to dissuade others from doing so. Switching around between the methods is relatively simple, due to the provided implementations from sci-py, and the fact, that we did not find improvements using them on our dataset does not necessarily have to mean there are no benefits when studying a different dataset.



\section{Prediction Architecture}
As we are using the sci-py implementation of the random forest model, we did not have to create architecture. Instead most work went into formatting and preprocessing the dataset. In order to give the model as much useful input information as possible we needed to provide the operator name, the input size and the GPU clock along with all potentially useful attributes of the operation's pytorch object to the predictor. \\
Because random forest model only takes numerical inputs, we make sure we format the input vector in a suitable way. For the operator names, this means we have a fixed number of categories in our dataset and can therefore use one-hot encoding to identify each type in a way that is readable to the random forest model. Another challenge arises from the fact, that different operations do not always have the same attributes. Along the same lines, the length of the input size tuple also is not equal for all operators. In order to preserve generality, we want to support all operators in one random forest model, which in turn means, we need to find a solution for this asymmetry in attributes for the different operators. The approach we ended up using, was to introduce a Boolean flag for each attribute entry in the input vector, signifying whether is applies for this operator.\\
For example, a linear layer expects an input tensor of the shape: \texttt{(batch\_size, in\_features)}, but a \texttt{Conv2d} layer expects: \texttt{(batch\_size, in\_channels, height, width)}. With that, the input size tuple for for a linear layer with \texttt{(batch\_size=32, input\_features=128)} would be encoded as \texttt{(32,1,128,1,-1,0,-1,0)}, whereas the input size tuple for a \texttt{Conv2d} layer with \texttt{(batch\_size=32, in\_channels=16, height=256, width=256)} would result in \texttt{(32,1,16,1,256,1,256,1)}. This way we can construct a meaningful input vector for the random forest model, which has a constant length and meaningful entries. We use -1 as our entry for non applying fields, as there are no negative input sizes. \\
In a similar fashion, there is a field in the input vector for \texttt{stride}. For convolutions, this carries the information of the stride, but for other operators like  linear layers or ReLUs, it simply carries a minus one, with a zero flag in the next entry signifying it does not apply for this operator. \\
This encoding approach is used on predefined list of attributes which are likely to have an impact on the computational characteristics of the operation. The choice of these parameters was conducted in a heuristic fashion, based on our understanding of the operators in question. However, further work could try to minimize the number of necessary attributes by quantitatively investigating their computational impact and including or excluding them accordingly. With the heuristic approach used here, we have attempted to err on the side of rather including too many attributes then to few. \\
The last important input vector entry we need to mention is the GPU clock speed. Even though the initial implementation trained runtime and power random forest predictors for each individual clock speed in our dataset, treating the GPU clock as a parameter for the model reduces unnecessary complexity without having a measurably negative impact on the prediction accuracy. This way it is just another parameter for the predictor, simplifying both the training of the random forest models, as well as their utilization, since it is no longer necessary to locate the specific model for the GPU clock speed of interest, and can instead just be fed into the predictor.


\section{Prediction Workflow}
As apparent from the architecture, our prediction model operates on the operations level. Is this is what is desired for a given study, that is great. One can simply provide the operations of interest with their respective input size and GPU clock to preprocessing and prediction script and will receive predictions. \\
Nevertheless, we expect most users to want to make predictions for whole neural networks. In that case the workflow is very similar to the dataset creation. It starts with extracting the operations, and how often they occur. Afterwards all unique operations will be ran through the predictor and the results summed up according to how often they occur in the model.\\
Critically, due to the modular nature of the approach, there is no need to the neural network being predicted to be actually executed or even exist in a finished form. As long as the operations are available it and it is known how often they occur predictions can be made. 