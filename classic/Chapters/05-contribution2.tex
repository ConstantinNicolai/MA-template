\chapter{Second Contribution}\label{chap:contrib2}

In this chapter we will introduce our model for providing predictions based on the collected dataset. We will go into the idea and decisions in creating it in this specific way and provide insight into the implementation.


\section{Model Selection}
Since we are interested in time and energy, we need two prediction models. More precicely, we need one for runtime predictions and one for power predictions. Multiplying the two predictions we obtain the our predicted energy. \\
In order to retain as much understanding of the prediction process as we can while ensuring tolerable execution times for the predicion times, we do not want to use a full DNN to perform the predictions. Instead we are looking for a more lightweight solution, closer to classical statisics. These requirements led us to using a random forest predictor model. It is both lightweight and sets restrictions to its input which force us to format our dataset in a way that provides some insight into cause and effect for the predictions. \\
From the same family of tree based ensemble methods, we also ran some tests with Extremely Randomized Trees, but the increased randomness did not yield better results. We therefore continued with the less complex random forest. A similar attempt was made with Extreme Gradient Boosting, but this also did not provide and increase in prediction accuracy justifying the increased training time. While we did not observe any reason to use on of these methods, we do not want to dissuade others from doing so. Switching around between the methods is relatively simple, due to the provided implementations from sci-py, and the fact, that we did not find improvements using them on our dataset does not necessarily have to mean there are no benefits when studying a different dataset.



\section{Prediction Architecture}
As we are using the sci-py implementation of the random forest model, we did not have to create architecture. Instead most work went into formatting and preprocessing the dataset. In order to give the model as much useful input information as possible we needed to provide the operator name, the input size and all potentially useful attributes of the operation's pytorch object to the predictor. \\
Because a random forest model only takes nummerical inputs, we make sure we format the input vector in a suitable way. For the operator names, this means we have a fixed number of categories in our dataset and can therefore use onehot encoding to identify each type in a way that is readable to the random forest model. Another challenge arises from the fact, that different operations do not always have the same attributes. Along the same lines, the length of the input size tuple also is not equal for all operators. In order to preserve generality, we want to support all operators in one random forest model, which in turn means, we need to find a solution for this asymetry in attributes for the different operators. The approach we ended up using, was to introduce a boolean flag for each attribute entry in the input vector, signifying whether is applies for this operator. For example, a linear layer expects an input tensor of the shape: \texttt{(batch\_size, in\_features)}, but a \texttt{Conv2d} layer expects: \texttt{(batch\_size, in\_channels, height, width)}. With that an the input size tuple for for a linear layer with \texttt{(batch\_size=32, input\_features=128)} would be encoded as \texttt{(32,1,128,1,-1,0,-1,0)}, whereas the input size tuple for a \texttt{Conv2d} layer with \texttt{(batch\_size=32, in\_channels=16, height=256, width=256)} would result in \texttt{(32,1,16,1,256,1,256,1)}. This way we can construct a meaningful input vector for the random forest model, which has a constant length and meaningful entries. We use -1 as our entry for non applying fields, as there are no negative input sizes. 