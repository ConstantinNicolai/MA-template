\chapter{Predictor Models}\label{chap:contrib2}

In this chapter we will introduce the regression model used for providing predictions based on the collected dataset. We will go into the idea and decisions in creating it in this specific way and provide insight into the implementation.


\section{Model Selection}
Since we are interested in predicting time and energy, we need two dedicated prediction models. More precisely, we need one model for runtime predictions and one for power predictions. By multiplying the two predictions we obtain our energy predictions. \\
In order to enable sufficient interpretability of the predictions, a high level of transparency into the prediction process is desirable. At the same time, we need to limit the predictor latency to ensure using the predictor exhibits lower latency than simply profiling the same workload instead. \\
Since using a DNN as our predictor fails to meet those requirements, we decided to use a random forest regressor. It includes decision paths allowing the interpretation of individual predictions and feature importance metrics which provide insight into the overall predictor behavior. Additionally it is also a computationally much more lightweight solution, because it utilizes orders of magnitude fewer parameters. \\
From the same family of tree based ensemble methods, we also ran some tests with an extremely randomized trees regressor, but did not find improved prediction results over the random forest. \\
A similar attempt was made with an extreme gradient boosting regressor. Using XGBoost resulted in similar performance to the random forest model. Each model outperformed the other one is some settings. For the sake of simplicity we will continue forward with the random forest model. It is both simpler in concept and exhibits better scalability characteristics due to its inherent parallelization.



\section{Predictor Architecture}

\begin{figure}
\texttt{[0 1 3 1 64 1 7.0 7.0 1.0 3.0 3.0 1.0 2.0 2.0 1.0 -1 0 -1 0 -1 0 -1 0 -1.0 0 -1.0 0 -1 0 -1.0 0 -1.0 0.0 -1.0 0.0 -1.0 0.0 -1.0 0.0 -1.0 -1.0 0.0 False False False True False False False False False False False False 32.0 1.0 3.0 1.0 224.0 1.0 224.0 1.0 1440]}
\caption{Example of the input vector formatting for the random forest predictor.}
\label{input_vector}
\end{figure}

We are using an existing random forest implementation provided by sklearn\footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{sklearn random forest implementation} }. Therefore most work went into formatting and preprocessing the dataset. In order to give the model as much useful input information as possible we needed to provide the operator name, the input size and the GPU clock along with all potentially useful attributes of the operation's pytorch object to the predictor. \\
Because random forest models only use numerical inputs, we made sure to format the input vector in a suitable way. The final format can be seen in Figure \ref{input_vector}.\\
For the operator names, this means we have a fixed number of categories in our dataset and can therefore use one-hot encoding to identify each type in a way that is readable to the random forest model. Another challenge arose from the fact, that different operations do not always have the same attributes. Along the same lines, the length of the input size tuple also is not equal for all operators. We do however want to support all operators in one predictor model, which in turn means, we needed to find a solution for this asymmetry in attributes for the different operators. The approach we ended up using, was to introduce a boolean flag for each attribute entry in the input vector, signifying whether is applicable for this operator.\\
For example, a linear layer expects an input tensor of the shape: \texttt{(batch\_size, in\_features)}, but a \texttt{Conv2d} layer expects: \texttt{(batch\_size, in\_channels, height, width)}. With that, the input size tuple for for a linear layer with \texttt{(batch\_size=32, input\_features=128)} would be encoded as \texttt{(32,1,128,1,-1,0,-1,0)}, whereas the input size tuple for a \texttt{Conv2d} layer with \texttt{(batch\_size=32, in\_channels=16, height=256, width=256)} would result in \texttt{(32,1,16,1,256,1,256,1)}. This way we can construct a meaningful input vector for the random forest model, which has a constant length and meaningful entries. We use -1 as our entry for non applying fields, as there are no negative input sizes. For the flags, a 1 signifies the entry being applicable, while a 0 signifies it not being applicable.\\
In a similar fashion, there is a field in the input vector for \texttt{stride}. For convolutions, this carries the information of the stride, but for other operators like  linear layers or ReLUs, it simply carries a minus one, with a zero flag in the next entry signifying it does not apply for this operator. \\
This encoding approach is used on a predefined list of attributes which are likely to have an impact on the computational characteristics of the operation. The choice of these parameters was conducted in a heuristic fashion, based on our understanding of the operators in question. However, further work could try to minimize the number of necessary attributes by quantitatively investigating their computational impact and including or excluding attributes accordingly. With the heuristic approach used here, we have leaned towards over-inclusion of attributes rather than under-inclusion to ensure inclusion of all relevant attributes. \\
The last important input vector entry we need to mention is the GPU clock speed. The initial implementation used to train runtime and power predictors for each individual clock speed in our dataset. However, treating the GPU clock as a parameter for the model reduces unnecessary complexity without negatively impacting the prediction accuracy. Treating it as an additional parameter for the monolithic runtime and power predictors simplifies both the training process, as well as the utilization of the predictor models, since the monolithic predictors eliminate the need to select the appropriate clock speed specific predictor model.

% This way it is just another parameter for the predictor, simplifying both the training of the random forest models, as well as their utilization, since it is no longer necessary to locate the specific model for the GPU clock speed of interest, and can instead just be fed into either the inference or the training predictor.


% As we are using the \texttt{sklearn} implementation of the random forest model\footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{sklearn random forest implementation} }, we did not have to create the architecture.

\section{Neural Network Level Predictor}
Our prediction models work on the operations level. This makes them directly applicable for any applications requiring operations level predictions. \\
However, most applications are expected to require predictions for full neural networks. In these cases the workflow is very similar to the dataset creation workflow. All operations and their number of occurrences are extracted from the neural network. After the whitelist filtering the predictor provides results for each unique operation. Those are then aggregated according to their number of occurrences in order to provide a prediction for the complete neural network. \\
Critically, due to the modular nature of the approach, there is no need for the neural network being predicted to be actually executed or even exist in functioning form. As long as the operations are available and it is known how often they occur predictions can be made.




% In case this is what is desired for a given study, the current state is already sufficient. One can simply provide the operations of interest with their respective input size and GPU clock to preprocessing and prediction script and will receive predictions. \\
% Nevertheless, we expect most users to want to make predictions for full neural networks. In that case the workflow is very similar to the dataset creation. It starts with extracting the operations and how often each operation occurs. Afterwards all unique operations will be run through the predictor and the results will be summed up according to their number of occurrences within the model.\\