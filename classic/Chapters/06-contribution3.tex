\chapter{Validation}\label{chap:contrib3}

While the previous contributions provided insight into the building blocks of this work, this chapter will serve to present its results. By providing quantitative results we can validate the methodology and provide an informed impression of both its capabilities and limitations. 


\section{Dataset Validation}
In this first section our focus is to ensure the datasets we collect for training are reasonable accurate. This is necessary in order to prevent the introduction of a strong bias due to dataset inaccuracies.
% This way we prevent basing our predictions on a training set which is skewed from the very start.
% Otherwise we would base our predictions on a training set which might be completely of the mark, defeating the purpose of the predictions from the very start. 

\subsection{Methodology}
Our measurements of the individual operations should resemble their execution in a DNN. The only way to ensure this is to compare them to the execution of a complete DNN. Since our operations are building blocks of DNNs, we cannot compare a single operation to a DNN. Rather we sum up our individually measured operations that build up a DNN and compare the resulting sum to a measurement of the complete DNN. \\
Our measurements for the complete DNN executions are performed using the same pipeline used to measure the individual operations. This is desirable because using the same pipeline for all measurements ensures comparable results. \\
% Using this pipeline is possible because the script can simply view a complete DNN as one larger operation to be profiled.\\
We also built the script that extracts which unique operations are present in a specific DNN in a way that is also tracks how often each unique operation occurs. This way, we can sum the results from our collected dataset of operations accordingly.


\subsection{Hardware Platforms}
% We begin by looking into our findings for our three original GPU configurations. In a later part we will look into results for different clock speeds.\\
The two hardware platforms studied here are the Nvidia RTX 2080 TI and the Nvidia A30. The Nvidia RTX 2080 TI is based upon the Turing architecture from the year 2018 and features 4352 CUDA cores and 544 first generation tensor cores with FP16 support. The Nvidia A30 is based upon the Ampere architecture form the year 2020 and features 3584 CUDA cores and 224 second generation tensor cores with TF32 support.\\
Given the capability of floating point 32 computation on the A30's tensor cores, we decided to probe its performance characteristics between having its tensor cores enabled and disabled. We cannot make the same differentiation for the 2080TI, because its tensor cores do not support \texttt{FP32}, which we use in all our benchmarks. \\
This leaves us with three configurations for the dataset validation. The 2080TI with default settings, the A30 with default settings and the A30 with its tensor cores disabled.


\subsection{Results}

% \subsubsection{RTX 2080 TI}
\textbf{RTX 2080 TI} Our results for the 2080TI our findings are not perfect. Agreement between measured and summed results does look rather promising for larger model-input sets. However, for smaller ones, there are instances where the agreement is less than ideal. The model-input sets displaying this behavior are the EfficientNetB0 (32, 3, 224, 224), the ResNet18 (32, 3, 32,32) and the ResNet34 (32, 3, 56, 56). In these instances, the summation overestimates both runtime and energy. However, the overestimation is more pronounced for runtime than for energy. \\
 

% \subsubsection{A30 Tensor Cores Disabled}
\textbf{A30 Tensor Cores Disable} Our results for the A30 with its tensor cores disabled are already more precise than the 2080TI's. While the same trends are visible, they are much less pronounced and our summation yields a closer approximations of the measurements overall. 


\subsubsection{A30 with Tensor Cores Enabled}
Our results for the A30 with its tensor cores enabled are very promising. While the earlier trends did not completely vanish, they are even less pronounced than for the A30 with disabled tensor cores. This hardware configuration yielded the most precise summation of the three configurations.

\subsubsection{Runtime Uncertainty Behavior}
For all runtime results, the standard deviation is very small, both for the summation and for the measured runtimes. Given that, it is clear that the discrepancies between the two cannot be solely caused by statistical noise. \\
Unfortunately, since this dataset validation is not they primary focus of this work, a deeper dive into its error estimation falls outside the scope.

\begin{figure}
    \center\includegraphics[width=0.82\textwidth]{Plots/tc_compare/all_three_small.pdf}
    \caption{Comparison of energy measurements for the 2080TI and the A30 with tensor cores once disabled and once enabled. The resulting ordering is identical for all model-input sets. However, the relative differences show a lot of variation, being more pronounced for these smaller model-input sets.}
    \label{fig:tcnotcsmall}
% \end{figure}
% \begin{figure}
    \includegraphics[width=0.82\textwidth]{Plots/tc_compare/all_three_large.pdf}
    \caption{For these larger model-input sets, we maintain the ordering, but we observe far weaker relative differences between the hardware configurations. For model-input sets with a high dependency complexity, like the ConvNext Base model, we find the most similar energy results across the configurations.}
    \label{fig:tcnotclarge}
\end{figure}


\newcounter{savedpage}
\setcounter{savedpage}{\value{page}}


\includepdf[pages=-, pagecommand={}]{Plots/conference_pages.pdf}



\setcounter{page}{\numexpr\value{savedpage}+4\relax}


\subsection{Tensor Core Real-World Impact}
As can be seen in Figure \ref{fig:tcnotcsmall} and Figure \ref{fig:tcnotclarge} showing the measured energy for the full model-input set runs on all three GPU configurations, tensor cores do have a significant impact on the energy efficiency of running PyTorch models. \\
This difference is more pronounced for smaller model-input sets and appears to become continually smaller for larger and more complex ones. But the difference does not appear to simply be proportional to the model's energy cost either. At first glance and without studying the individual model architectures in detail, it would appear that the difference decreases with the model's dependency complexity. \\
Dependency complexity is used here to describe both the amount and the depth of dependencies, measured by the number of layers they span, when dependencies go beyond direct, sequential connections between adjacent layers.\\
When comparing the results for different flavors of ResNets to the results for model architectures with higher dependency complexity such as ConvNext, EfficientNet and DensetNet, it can be seen that the results are much closer for the latter ones, while for the ResNets the tensor cores get to show their potential. \\
Taking a step back from studying the impact of the tensor cores, there are also interesting findings in comparing the results for the 2080TI to the other GPU configurations. We find worse energy efficiency for the 2080TI compared to the A30 running with tensor cores for all models. But when the tensor cores are disabled this trend gets reversed. Overall the difference between the 2080TI and the A30 without tensor cores is smaller than the difference between the 2080TI and the A30 with its tensor cores enabled. However, the pattern of the energy efficiency being best on the A30 with tensor cores, the 2080TI occupying the middle position, and the A30 without tensor cores having the worst energy efficiency remains the same for all model-input sets.


\section{Prediction Accuracy}

With this section we are moving on from results and insight gained directly from the dataset collection and move into our findings for the prediction model.

\subsection{Operations Level}

Because the prediction model works on the operations level, we will begin by evaluating it on the operations level. \\
We are using the implementations from the \texttt{sklearn} library and have investigated XGBRegressor, ExtraTreesRegressor and RandomForestRegressor to use as our prediction model. Initial tests showed clearly that ExtraTreesRegressor performed worse than the others in terms of predictive power, so we focused on the other two. \\
We chose to use the coefficient of determination, also known as $R^2$, as our metric to evaluate the prediction accuracy. In order to provide a more well rounded representation, both the $R^2$ score for a test set seperate from the training set, as well as the mean $R^2$ score for a 15 fold cross validation are given. \\
Because the inference and training datasets are conceptually separate and measured independently, the prediction models are separate as well. Naturally,  their evaluation is split up too.


\subsubsection{Training A30}



\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Training A30}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.876 \pm 0.073$ &  $0.912 \pm 0.04$ \\
\hline
Test Set $R^2_{time}$ & $0.9036$ & $0.9018$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.977 \pm 0.007$  &  $0.985 \pm 0.003$\\
\hline
Test Set $R^2_{power}$ & $0.9797$ & $0.9868$ \\
\hline
\end{tabular}
\caption{These operations level results for this predictor are very good. With scores of around $0.9$ for the runtime predictor and around $0.98$ for the power predictor, its performance is very respectable. The XGBoost predictor shows marginally better performance to the random forest one, but the results are very close.}
\label{tab:pred_res}
\end{table}


The resulting values for the model predicting training performance can be found in Table \ref{tab:pred_res}. Keep in mind that the scores show a small amount of variation depending on the random seed used for the predictions models and the test set, training set split. \\
As can be seen from the results in the table, the XGBoost model preforms marginally better in terms of prediction accuracy. Unfortunately, this was only discovered very late in the research process, because it was hidden in the random seed uncertainty. As further work was already conducted with the random forest model, a pivot back would go cost a lot of time. Additionally, the prediction performance may be worse, but not by a large margin. Therefore further results shown were acquired using the random forest model. This also provides the advantage of using a model which is more simple in nature and should exhibit better scalability properties in general. Investigations into which model can provide predictions with lower latency could also be part of future work. \\
We also included Figure \ref{fig:testsetoperations} in order to convey a visual impression of the prediction performance. For a random subset of operations from the test set, it shows the predicted value alongside the measurement value. \\
Both the $R^2$ score results as well as the visual interpretation tell a similar story. We clearly have a stronger predictive power for the power model than for the runtime model. The runtime model still provides decent results, but they are not on the same level as the power model. Both prediction performance and prediction latency requirements will determine whether the performance we can provide is suitable for any given application in the end.


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Operation Predictions Training Model}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/stacked_comparison.pdf}}
    \caption{Comparison of predictions to measurements for ten operations from the test set for the A30 training prediction model. While we have outstanding agreement for most of them, we can see the limits of our predictive power in the results for operations 2 and 9. This illustration can give us a more intuitive impression of the $R^2$-error of 0.87 for the runtime and of 0.97 for the power.}
    \label{fig:testsetoperations}
\end{figure}


\subsubsection{Inference A30}

The resulting values for the model predicting inference performance can be found in Table \ref{tab:pred_res_inf}. Our resulting $R^2$ scores lie in a very similar regime as the ones for the training model. \\
If we were to attribute the lower predictive performance of the runtime model with XGBoost and for the random forest model test set score to more than the inherent uncertainty, it might be explained by the smaller absolute runtimes for inference. Smaller runtimes with the same measurement methodology should result in similar absolute uncertainties and therefore in larger relative uncertainties. These larger relative uncertainties might play a part in the slightly weaker predictive performance we observe for the inference predictor.\\
All power model scores both for the random forest model, as well we the XGBoost model show practically identical performance to the training model, taking into consideration the additional uncertainty introduced by the random seed affecting the predictive performance slightly. \\
Given the very similar results, both in these metrics and in the visual interpretation of Figure \ref{fig:testsetops_inf}, our conclusions for the inference model are the same as for the training model. The lower predictive power of the runtime model compared to the power model will result in a limit of its suitable applications at some point, but its performance is still acceptable. 



\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Inference A30}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.890 \pm 0.66$ &  $0.898 \pm 0.05$ \\
\hline
Test Set $R^2_{time}$ & $0.8827$ & $0.8924$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.981 \pm 0.003$  &  $0.988 \pm 0.002$\\
\hline
Test Set $R^2_{power}$ & $0.9853$ & $0.9895$ \\
\hline
\end{tabular}
\caption{Our inference predictor for the A30 does very well. With runtime scores just below $0.9$ and power scores higher than $0.98$, it is performs just as well as the A30 training predictor. }
\label{tab:pred_res_inf}
\end{table}



\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Operation Predictions Inference Model}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/stacked_comparison_inf.pdf}}
    \caption{Comparison of predictions to measurements for ten operations from the test set for the A30 inference prediction model. The different orders of magnitude make the plot challenging to read, but we can see decent agreement between predictions and measurements. We do however see slight overpredictions for the resulting energy for operations 1 and 7.}
    \label{fig:testsetops_inf}
\end{figure}


\subsubsection{Training RTX2080TI}

The prediction model for the RTX2080TI is not built to predict for specific clock speeds. Instead it is trained on a dataset collected running the GPU in its default configuration. Therefore that is also what it predicts for. Unfortunately, we find worse prediction performance than for our A30 predictors. The results can be seen in table \ref{tab:pred_res_2080}. They are not unusable, but disappointing in comparison. Interestingly however, this predictor breaks the pattern of XGBoost always performing a little better. Here we find a few cases where it performs better and a few where it performs worse. This might come down to the property of random forest models of generalizing pretty well, even with a smaller amount of training data. Since this is a dataset for the default auto clock speed, it is several times smaller then the dataset for multiple clock speeds. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Training 2080TI}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.755 \pm 0.122$ &  $0.684 \pm 0.359$ \\
\hline
Test Set $R^2_{time}$ & $0.813$ & $0.793$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.920 \pm 0.058$  &  $0.921 \pm 0.054$\\
\hline
Test Set $R^2_{power}$ & $0.866$ & $0.869$ \\
\hline
\end{tabular}
\caption{The training predictor for the 2080TI scores notably lower than the A30 predictors. It also reverses the A30 predictor trend of the XGBoost models performing slightly better. For the 2080TI predictors, XGBoost performs a little worse. The smaller training set due to the lack of a clock speed study for the A30 and the smaller focus on consistency of a consumer GPU compared to the A30 might contribute to the weaker predictor performance here. Still, with a runtime score of over $0.75$ and a power score of around $0.9$ it is far from unusable.}
\label{tab:pred_res_2080}
\end{table}

\subsubsection{Inference RTX2080TI}

The resulting $R^2$ scores for our 2080TI inference predictor can be seen in table \ref{tab:pred_res_2080_inf}. For the runtime models we see a steep drop in $R^2$ score from the random forest model to the XGBoost model. But even for the better performing random forest model, this is still the worst performing predictor out of our four predictor models. This might be caused by a combination of multiple factors. It is trained on a smaller dataset, since it is not a multi clock model and it is an inference predictor, which means it has to predict smaller absolute values, which will have larger relative errors in the training set. Lastly, since the 2080TI is a consumer GPU and not a datacenter GPU, its behavior is tuned to focus on the best burst load performance, but not necessarily on the greatest consistency and stability compared to a datacenter GPU such as the A30.


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Inference 2080TI}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.735 \pm 0.088$ &  $0.607 \pm 0.079$ \\
\hline
Test Set $R^2_{time}$ & $0.769$ & $0.613$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.939 \pm 0.034$  &  $0.944 \pm 0.026$\\
\hline
Test Set $R^2_{power}$ & $0.908$ & $0.885$ \\
\hline
\end{tabular}
\caption{The inference predictor for the 2080TI is our worst performer. It shares the challenges of the 2080TI training predictor, while having to work with a dataset of smaller values, which naturally have larger relative errors. Here the XGBoost predictor for runtime scores so much lower that there is not question in using the random forest one. Sticking to the random forest predictors here, we have a runtime score of $0.73$ and a power score of $0.94$, which is lower than we would like but still decent enough.}
\label{tab:pred_res_2080_inf}
\end{table}


\subsection{Neural Network Level}


In this next step, we will evaluate the prediction performance of the random forest model for the A30 on the neural network level. \\
% We will perform an more detailed analysis on this part, as this is the abstraction level which is most commonly used and therefore the most relevant to provide predictions for. \\
Since this prediction model is capable of providing predictions for different clock speeds, we will conduct the validation for each clock speed. As this is a graphical validation, we will provide the measured results for each clock speed with the predicted results below. This way both the direct comparison between measurement and prediction is visible, as well as the behavior across different clock speeds. \\
Results are given for both runtime and energy, as well as for both inference and training. Additionally, plots of the energy delay product are provided to illustrate a trade-off between optimizing for either time or energy on their own. For this, the results for each model are normalized, because the orders of magnitude between the different models are to large for this plot to remain readable otherwise. \\
% Furthermore, not as a preplanned point of interest for this work, but rather as a fortunate byproduct of the necessary measurements we have also found some patterns in the behavior across clocks, which we will discuss for too.
\ref{fig:grpfmtimeinf} As expected for the inference runtime measurements, the runtime decreases with an increase in clock speed for all tested models. \\
\ref{fig:gprpredtimeinf} The corresponding inference runtime predictions manage to maintain the ordering between different clock speeds.
% While the absolute results for the predictions may be close, but not perfect, the ordering between different clock speeds is maintained flawlessly from the measurement results. 
This allows the predictions to be used to determine the optimal clock speed. \\
\ref{fig:grpfmenergyinf} As opposed to the monotonous relationship between runtime and clock speed, the optimal clock speed for the inference energy measurements is $900$ MHz. \\
\ref{fig:fprpredenergyinf} As expected from the operations level results, we can see that the predictions for the inference energy are closer to their corresponding measurements than the inference runtime predictions are to their corresponding measurements. They also maintain the clock speed optimum found in the measurements, making these predictions suitable for energy optimizations. \\
\ref{fig:grpfmtimetrain} For the training runtime measurements we observe the same behavior we saw in the inference case. Lower clock speeds lead to longer runtimes.\\
\ref{fig:grppredenergytrain} In the training runtime predictions, we see a similar prediction performance to the inference case. The predictions provide the correct ordering in this case too.\\
\ref{fig:grpfmenergytrain} Identically to the inference case, the optimal clock speed in terms of training energy measurement lies at 900 MHz. The absolute energy costs for training are between 2 and 5 times as high as the inference costs. \\
\ref{fig:grppredenergytrain} Comparing these training energy predictions to the corresponding measurements just above illustrates how capable our predictions are at reproducing the real world behavior. These predictions are the most precise ones among the presented cases. The ordering is maintained in this case as well.\\
\ref{fig:grppdpfminf} Different from the 900 MHz optimum for energy, we find the optimum for the inference measurement of the energy delay product at 1200 MHz. Being the product of runtime and energy it provides a more balanced metric to optimize along. \\
\ref{fig:grppdppredinf} For the inference predictions of the energy delay product, we find larger discrepancies between measurement and prediction, than for the energy predictions. From the operations level evaluation, we know the power predictions perform better than the time predictions. The fact that time contributes to the energy delay product both as a factor in the energy and then again being multiplied with the energy prediction to form the prediction of the energy delay product, explains the observed larger discrepancies. \\
% between measurement and prediction here, than for the energy results. \\
\ref{fig:grppdpfmtrain} For the training measurements of the energy delay product, there is not a lot of change to be observed compared to the inference measurement results in the normalized plot. This indicates a roughly proportional scaling of the energy delay product between inference and training. \\
\ref{fig:grppdppredtrain} In the training predictions of the energy delay product, we see much larger deviations between measurement and predictions compared to the inference case. Runtime predictions are less precise than power predictions. Absolute times are several times larger for training than for inference. This effect is amplified by the squared contribution of time towards the energy delay product. This squared contribution of a larger absolute value which is less precise causes the larger deviations.



\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Time per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_time_across_clocks_inference.pdf}}
    \caption{Inference time measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmtimeinf}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Time per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_time_across_clocks_inference.pdf}}
    \caption{Inference time predictions for 18 model-input sets across six clock speeds.}
    \label{fig:gprpredtimeinf}
\end{figure}


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Energy per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_energy_across_clocks_inference.pdf}}
    \caption{Inference energy measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmenergyinf}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Energy per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_energy_across_clocks_inference.pdf}}
    \caption{Inference energy predictions for 18 model-input sets across six clock speeds.}
    \label{fig:fprpredenergyinf}
\end{figure}


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Time per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_time_across_clocks_training.pdf}}
    \caption{Training time measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmtimetrain}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Time per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_time_across_clocks_training.pdf}}
    \caption{Training time predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppredtimetrain}
\end{figure}


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Energy per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_energy_across_clocks_training.pdf}}
    \caption{Training energy measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmenergytrain}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Energy per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_energy_across_clocks_training.pdf}}
    \caption{Training energy predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppredenergytrain}
\end{figure}



\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Normalized Energy Delay Product per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_pdp_inference.pdf}}
    \caption{Inference energy delay product measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grppdpfminf}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Normalized Energy Delay Product per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_pdp_inference.pdf}}
    \caption{Inference energy delay product predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppdppredinf}
\end{figure}



\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Normalized Energy Delay Product per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_pdp_training.pdf}}
    \caption{Training energy delay product measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grppdpfmtrain}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Normalized Energy Delay Product per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_pdp_training.pdf}}
    \caption{Training energy delay product predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppdppredtrain}
\end{figure}

\FloatBarrier

\subsubsection{Patterns and Observations}
While we are not surprised by the runtime measurement results simply exhibiting shorter runtimes for higher clock speeds, we do find an interesting pattern for the energy measurement results. Both for training and inference we observe a "U" pattern across the clock speeds, see Figure \ref{fig:pattern}. With very high energy costs for very low clock speeds and high energy costs for very high clock speeds as well. But in between, for higher, but not very high clock speeds, we see a minimum for the required energy at a clock speed of $900$ MHz on the A30. More interestingly this pattern remains stable across both training and inference and for all models tested. Therefore, even if there might be models where it is not optimal, it is surely a good rule of thumb to use this as a starting point when energy efficiency is of interest. \\
When trying to optimize for a balance between energy and runtime, we observe a shift to the right. Looking at the energy delay product, see Figure \ref{fig:grppdpfminf} and Figure \ref{fig:grppdpfmtrain}, we see similar "U" pattern in the measurements, with the optimum at $1200$ MHz across inference and training. \\
Unfortunately, we do not see this pattern reproduced perfectly by our predictions for the training case. Since the runtime has a squared contribution to the energy delay product and the absolute values for training are larger, this combination leads to a scenario where our prediction precision for the runtime becomes insufficient to predict the correct optimum. 


\begin{figure}[htbp]
    \centering
    \parbox{0.7\textwidth}{\centering\textbf{Energy Cost "U" Pattern}}
    \makebox[\textwidth]{\includegraphics[width=0.7\textwidth]{Plots/example_model_plot.pdf}}
    \caption{Looking at the energy measurements across different clock speeds for a single model, the "U" pattern which is present for all models becomes even more apparent. In terms of energy cost it is most efficient to run both inference and training tasks at a clock speed of $900$ MHz for all our tested models.}
    \label{fig:pattern}
\end{figure}

\section{Predictor Latency}


After a few more tweaks to improve performance, we arrived at a loading time of around 30 seconds for the full set of models used to validate the predictor. These 30 seconds are in a completely different order of magnitude in terms of execution time compared to preprocessing, prediction of the models or even execution of the models. Realistically, this time is more comparable to having to download a model before being able to benchmark it. The next largest contributor in terms of execution time is preprocessing the dataset in order to format is in a way that can be used as an input for our predictor model. In its current form this step takes somewhere around 500 ms. For the full validation set of 18 model-input sets it takes around 600 ms. For an individual model it takes less than 500 ms. The fact that this step is relatively speaking so slow is unfortunate. This is a result of this being research code which is not optimized. While it is not possible to predict how much headroom for optimization there is, some improvements should be possible, if this were to go to production. \\
The most important evaluation in terms of predictor execution time is however the execution time of the actual predictor model. Predicting time and energy at a specific clock speed for all model-input sets in the validation set shows very good performance. Across multiple clock speeds and for both for the training predictor and the inference predictor the resulting execution time hovers around 30 ms. Even for the scenario with the smallest execution time, inference at maximum clock speed, this still beats the collective execution time of around 300 ms of the validation set by one order of magnitude. And this comparison shifts only further in favor of the predictor model when we look at a still very reasonable scenario of training at a clock speed of 930 MHz. With this the collective execution time is close to 2000 ms, while the predictor time is not affected. Here we have an predictor latency over 60 times lower than performing the measurement instead.