\chapter{Validation}\label{chap:contrib3}

While the previous contributions provided insight into the building blocks of this work, this chapter will serve to present its results. By providing quantitative results we are validating the methodology and providing an informed impression of both its capabilities and limitations. 


\section{Dataset Validation}
Our focus in this first section is to ensure the datasets we collect for the training of our predictors are reasonably accurate. This prevents the introduction of a strong bias due to dataset inaccuracies.

% In this first section our focus is to ensure the datasets we collect for training are reasonable accurate. This is necessary in order to prevent the introduction of a strong bias due to dataset inaccuracies.
% This way we prevent basing our predictions on a training set which is skewed from the very start.
% Otherwise we would base our predictions on a training set which might be completely of the mark, defeating the purpose of the predictions from the very start. 

\subsection{Methodology}
% Our measurements of the individual operations should resemble their execution in a DNN. The only way to ensure this is to compare them to the execution of a complete DNN. Since our operations are building blocks of DNNs, we cannot compare a single operation to a DNN. Rather we sum up our individually measured operations that build up a DNN and compare the resulting sum to a measurement of the complete DNN. \\
Our measurements of the individual operations should resemble how they behave when executed within a complete DNN. Since our individual operations are merely 
building blocks, we cannot compare them directly to the entire neural network. Instead, we aggregate the measurements of the operations which compose the DNN and compare that total to the execution of the full model for validation. \\
Our measurements for the complete DNN executions are performed using the same pipeline used to measure the individual operations. This is desirable because using the same pipeline for all measurements ensures comparable results. \\
We also built a script that extracts which unique operations are present in a specific DNN in a way which also tracks how often each unique operation occurs. This way, we can sum the results from our collected dataset of operations accordingly.


% Using this pipeline is possible because the script can simply view a complete DNN as one larger operation to be profiled.\\

\subsection{Hardware Platforms}
% We begin by looking into our findings for our three original GPU configurations. In a later part we will look into results for different clock speeds.\\
The two hardware platforms studied here are the Nvidia RTX 2080 TI and the Nvidia A30. The Nvidia RTX 2080 TI is based upon the Turing architecture from the year 2018 and features 4352 CUDA cores and 544 first generation tensor cores with \texttt{FP16} support. The Nvidia A30 is based upon the Ampere architecture form the year 2020 and features 3584 CUDA cores and 224 second generation tensor cores with \texttt{TF32} support.\\
Given the capability of working with \texttt{FP32} values using the \texttt{TF32} datatype on the A30, we decided to probe its performance characteristics between having its tensor cores enabled and disabled. We cannot make the same differentiation for the 2080TI, because its tensor cores do not support \texttt{FP32} native nor \texttt{TF32} and all our benchmarks use FP32 values. \\
This leaves us with three configurations for the dataset validation. The 2080TI with default settings, the A30 with default settings and the A30 with its tensor cores disabled.


\subsection{Results}

% \subsubsection{RTX 2080 TI}
\textbf{RTX 2080 TI} Our results for the 2080TI are mixed. Agreement between measured and summed results does look rather promising for larger model-input sets. However, for smaller ones, there are instances where the agreement is weaker. The model-input sets displaying this behavior are the EfficientNetB0 (32, 3, 224, 224), the ResNet18 (32, 3, 32,32) and the ResNet34 (32, 3, 56, 56). In these instances, the summation overestimates both runtime and energy. However, the overestimation is more pronounced for runtime than for energy. \\
\textbf{A30 Tensor Cores Disabled} Our results for the A30 with its tensor cores disabled are already more precise than the 2080TI's. While the same trends are visible, they are much less pronounced and our summation yields a closer approximations of the measurements overall. \\
\textbf{A30 with Tensor Cores Enabled} Our results for the A30 with its tensor cores enabled are very promising. While the earlier trends did not completely vanish, they are even less pronounced than for the A30 with disabled tensor cores. This hardware configuration yielded the most precise summation of the three configurations.



\subsection{Uncertainties}
The standard deviation in our results is very small, both for the summation and for the measurements. It is clear that the discrepancies between the two cannot be solely caused by statistical noise. Systematic uncertainties are the larger contributor which dominates the remaining statistical noise in our measurements. For this reason, statistical errors are omitted in the evaluations following the dataset validation. The wide range of potential systematic sources makes the identification of specific systematic contributors very difficult. Since that was not required for the performance assessment of our predictor models it was not the focus of this project. 

% and was not the focus of this project. The prediction performance evalutation 


%  The specific systematic contributors were not able to be identified, because the wide range of potential sources made this task very difficult.


\newcounter{savedpage}
\setcounter{savedpage}{\value{page}}


\includepdf[pages=-, pagecommand={}]{Plots/plot_pages.pdf}



\setcounter{page}{\numexpr\value{savedpage}+3\relax}


\subsection{Tensor Core Real-World Impact}
As can be seen in Figure \ref{fig:tcnotcsmall} and Figure \ref{fig:tcnotclarge} showing the measured energy for the full model-input set runs on all three GPU configurations, tensor cores do have a significant impact on the energy efficiency of running PyTorch models. This is illustrated by the difference between the results for the A30 with tensor cores and without tensor cores. \\
This difference is more pronounced for smaller model-input sets and appears to become continually smaller for larger and more complex ones. But the difference does not appear to simply be proportional to the model's energy cost either. At first glance and without studying the individual model architectures in detail, it would appear that the difference decreases with the model's dependency complexity. \\
In this context, dependency complexity refers to the depth and quantity of dependencies in a model. Depth is defined as the number of layers a dependency spans when it goes beyond a sequential connection between adjacent layers. Skip connections are a prime example of this phenomenon. \\
When comparing the results for different flavors of ResNets to the results for model architectures with higher dependency complexity such as ConvNext, EfficientNet and DensetNet, it can be seen that the results are much closer for the latter ones, while for the ResNets the tensor cores get to show their potential. \\
Taking a step back from studying the impact of the tensor cores, there are also interesting findings in comparing the results for the 2080TI to the other GPU configurations. We find worse energy efficiency for the 2080TI compared to the A30 running with tensor cores for all models. When the tensor cores are disabled this trend gets reversed. Overall the difference between the 2080TI and the A30 without tensor cores is smaller than the difference between the 2080TI and the A30 with its tensor cores enabled. However, the pattern of the energy efficiency being best on the A30 with tensor cores, the 2080TI occupying the middle position, and the A30 without tensor cores having the worst energy efficiency remains the same for all model-input sets.


\begin{figure}
    \center\includegraphics[width=0.82\textwidth]{Plots/tc_compare/all_three_small.pdf}
    \caption{Comparison of energy measurements for the 2080TI and the A30 with tensor cores once disabled and once enabled. The resulting ordering is identical for all model-input sets. However, the relative differences show a lot of variation, being more pronounced for these smaller model-input sets.}
    \label{fig:tcnotcsmall}
% \end{figure}
% \begin{figure}
    \includegraphics[width=0.82\textwidth]{Plots/tc_compare/all_three_large.pdf}
    \caption{For these larger model-input sets, we maintain the ordering, but we observe far weaker relative differences between the hardware configurations. For model-input sets with a high dependency complexity, like the ConvNext Base model, we find the most similar energy results across the configurations.}
    \label{fig:tcnotclarge}
\end{figure}

\FloatBarrier


\section{Operations Level Predictions}

% With this section we are moving on from results and insight gained directly from the dataset collection and move into our findings for the prediction model.

% \subsection{Operations Level}

% Because the prediction model works on the operations level, we will begin by evaluating it on the operations level. \\
We begin our evaluation of the prediction performance by evaluating our prediction model on the operations level. \\
We are using the implementations from the \texttt{sklearn} and \texttt{xgboost} libraries and have investigated XGBRegressor\footnote{\href{https://xgboost.readthedocs.io/en/latest/python/index.html}{xgboost XGBRegressor} }, ExtraTreesRegressor\footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html}{sklearn Extra Trees Regressor} } and RandomForestRegressor\footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{sklearn Random Forest Regressor} } for use as our prediction model. Initial tests showed clearly that ExtraTreesRegressor performed worse than the others in terms of predictive power, so we focused on the other two. \\
We chose to use the coefficient of determination ($R^2$, \ref{R2}) as our metric to evaluate the prediction accuracy. \\
In order to provide a more well rounded representation, both the $R^2$ score for a test set separate from the training set, as well as the mean $R^2$ score for a 15 fold cross-validation are given. \\
The inference and training datasets are conceptually separate and measured independently. Therefore, the prediction models and their evaluation are seperate as well.


% therefore the prediction models are separate as well. Naturally, their evaluation is split up too.


% \footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestRegressor.html}{sklearn Random Forest Regressor} }


% \footnote{\href{https://xgboost.readthedocs.io/en/latest/python/python_api.html#xgboost.XGBRegressor}{xgboost XGBRegressor} }


% \footnote{\href{https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesRegressor.html}{sklearn Extra Trees Regressor} }


\subsection{Training A30}



\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Training A30}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.876 \pm 0.073$ &  $0.912 \pm 0.04$ \\
\hline
Test Set $R^2_{time}$ & $0.90$ & $0.90$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.977 \pm 0.007$  &  $0.985 \pm 0.003$\\
\hline
Test Set $R^2_{power}$ & $0.98$ & $0.99$ \\
\hline
\end{tabular}
\caption{Operations level results for the random forest and XGBoost A30 predictors. Cross-validation slightly favors XGBoost but the test set scores are stable across both regressors.}
\label{tab:pred_res}
\end{table}



% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
%  \textbf{Training A30}& \textbf{Random Forest} & \textbf{XGBoost} \\
% \hline
% CV $\overline{R^2}_{time}$ & $0.876 \pm 0.073$ &  $0.912 \pm 0.04$ \\
% \hline
% Test Set $R^2_{time}$ & $0.9036$ & $0.9018$ \\
% \hline
% CV $\overline{R^2}_{power}$ & $0.977 \pm 0.007$  &  $0.985 \pm 0.003$\\
% \hline
% Test Set $R^2_{power}$ & $0.9797$ & $0.9868$ \\
% \hline
% \end{tabular}
% \caption{Operations level results for the random forest and XGBoost A30 predictors. cross-validation and test set score are in close agreement.}
% \label{tab:pred_res}
% \end{table}


% These operations level results for this predictor are very good. With scores of around $0.9$ for the runtime predictor and around $0.98$ for the power predictor, its performance is very respectable. The XGBoost predictor shows marginally better performance to the random forest one, but the results are very close.

The resulting values for the model predicting training performance can be found in Table \ref{tab:pred_res}. The scores show a small amount of variation depending on the random seed used for the predictions models and the test set, training set split. \\
For the A30 training predictor, the XGBoost model performs slightly better in terms of prediction performance. However the random forest model has the advantage of being more simple in nature and is expected to exhibit better scalability properties in general. For this reason and in order to keep the predictor model constant between different predictors, all results past the operations level evaluation were acquired using the random forest model. \\
% As can be seen from the results in the table, the XGBoost model preforms marginally better in terms of prediction accuracy. Therefore further results shown were acquired using the random forest model. This also provides the advantage of using a model which is more simple in nature and should exhibit better scalability properties in general. Investigations into which model can provide predictions with lower latency could also be part of future work. \\
We also included Figure \ref{fig:testsetoperations} in order to convey a visual impression of the prediction performance. For a random subset of operations from the test set, it shows the predicted value alongside the measurement value. \\
Both the $R^2$ score results as well as the visual interpretation tell a similar story. We have more accurate predictions for the power model than for the runtime model. \\
For either model, prediction performance and prediction latency requirements will determine whether the performance we can provide is suitable for any given application.


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Operation Predictions Training Model}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/stacked_comparison.pdf}}
    \caption{Comparison of predictions to measurements for ten operations from the test set for the A30 training prediction model. While we have outstanding agreement for most of them, we can see the limits of our predictive power in the results for operations 1 and 6. This illustration can give us a more intuitive impression of the $R^2$-error of 0.90 for the runtime and of 0.98 for the power.}
    \label{fig:testsetoperations}
\end{figure}


\subsection{Inference A30}

The resulting values for the model predicting inference performance can be found in Table \ref{tab:pred_res_inf}. Our resulting $R^2$ scores are similar to the ones for the training model. \\
Both the runtime model with XGBoost and the runtime random forest test set evaluation score slightly lower than in the training case. One possible explanation originates from the smaller absolute runtimes for inference. Smaller runtimes with the same measurement methodology result in similar absolute uncertainties and therefore in larger relative uncertainties. These larger relative uncertainties cause the slightly weaker predictive performance we observe for the inference predictor.\\
% If we were to attribute the lower predictive performance of the runtime model with XGBoost and for the random forest model test set score to more than the inherent uncertainty, it might be explained by the smaller absolute runtimes for inference. Smaller runtimes with the same measurement methodology should result in similar absolute uncertainties and therefore in larger relative uncertainties. These larger relative uncertainties might play a part in the slightly weaker predictive performance we observe for the inference predictor.\\
Disregarding minor uncertainties introduced by the random seed, the random forest and XGBoost power models perform identically to the training case. \\
Given the very similar results, both in these metrics and in the visual interpretation of Figure \ref{fig:testsetops_inf}, our conclusions for the inference model are the same as for the training model. The lower predictive power of the runtime model compared to the power model will result in a limit of its suitable applications, but it remains application dependent. 


% All power model scores both for the random forest model, as well we the XGBoost model show identical performance to the training model, taking into consideration the additional uncertainty introduced by the random seed affecting the predictive performance slightly. \\

% The slightly lower scores of the runtime model with XGBoost and the random forest test set score compared to the training case are explained by inherent uncertainties. However one possible explanation going beyond uncertainty originates from the smaller absolute runtimes for inference.



\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Inference A30}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.890 \pm 0.66$ &  $0.898 \pm 0.05$ \\
\hline
Test Set $R^2_{time}$ & $0.88$ & $0.89$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.981 \pm 0.003$  &  $0.988 \pm 0.002$\\
\hline
Test Set $R^2_{power}$ & $0.99$ & $0.99$ \\
\hline
\end{tabular}
\caption{Operations level results for the random forest and XGBoost A30 predictors. Both cross-validation and test set scores are stable across both regressors.}
% \caption{Our inference predictor for the A30 works very well. With runtime scores just below $0.9$ and power scores as high as $0.99$, it is performs very similar to the A30 training predictor. }
\label{tab:pred_res_inf}
\end{table}


% \begin{table}[h!]
% \centering
% \begin{tabular}{|c|c|c|}
% \hline
%  \textbf{Inference A30}& \textbf{Random Forest} & \textbf{XGBoost} \\
% \hline
% CV $\overline{R^2}_{time}$ & $0.890 \pm 0.66$ &  $0.898 \pm 0.05$ \\
% \hline
% Test Set $R^2_{time}$ & $0.8827$ & $0.8924$ \\
% \hline
% CV $\overline{R^2}_{power}$ & $0.981 \pm 0.003$  &  $0.988 \pm 0.002$\\
% \hline
% Test Set $R^2_{power}$ & $0.9853$ & $0.9895$ \\
% \hline
% \end{tabular}
% \caption{Our inference predictor for the A30 does very well. With runtime scores just below $0.9$ and power scores higher than $0.98$, it is performs just as well as the A30 training predictor. }
% \label{tab:pred_res_inf}
% \end{table}


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Operation Predictions Inference Model}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/stacked_comparison_inf.pdf}}
    \caption{Comparison of predictions to measurements for ten operations from the test set for the A30 inference prediction model. The different orders of magnitude make the plot challenging to read, but we can see decent agreement between predictions and measurements. We do however see slight overpredictions for the resulting energy for operations 1 and 6.}
    \label{fig:testsetops_inf}
\end{figure}


\subsection{Training RTX2080TI}

The prediction model for the RTX2080TI does not provide predictions for specific clock speeds. Instead it is trained on a dataset collected running the GPU in its default configuration. Its prediction performance is not as strong as the A30 predictor. \\
The results can be seen in table \ref{tab:pred_res_2080}. This predictor breaks the pattern of XGBoost performing slightly better. We find both cases where it performs better and cases where it performs worse than the random forest. A possible explanation of this behavior the property of random forests to generalize well, even with smaller training sets. Compared to the A30 where the training set spans six clock speeds, this training set is six times smaller. 

% Here we find some cases where it performs better and some where it performs worse. A possible explanation for this is the property of random forest models of generalizing pretty well, even with smaller amounts of training data. This is the case here, since this is a dataset only for the default auto clock speed. 
% In comparison, the A30 datasets are six times as large, because they cover six different clock speeds.

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Training 2080TI}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.755 \pm 0.122$ &  $0.684 \pm 0.359$ \\
\hline
Test Set $R^2_{time}$ & $0.81$ & $0.79$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.920 \pm 0.058$  &  $0.921 \pm 0.054$\\
\hline
Test Set $R^2_{power}$ & $0.87$ & $0.87$ \\
\hline
\end{tabular}
\caption{Operations level results for the random forest and XGBoost 2080TI predictors. Test set scores are stable across both regressors, but the cross-validation indicates the random forest is the better choice.}



% The training predictor for the 2080TI scores notably lower than the A30 predictors. It also reverses the A30 predictor trend of the XGBoost models performing slightly better. For the 2080TI predictors, XGBoost performs a little worse. The smaller training set due to the lack of a clock speed study for the A30 and the smaller focus on consistency of a consumer GPU compared to the A30 might contribute to the weaker predictor performance here. It results in a runtime score of over $0.75$ and a power score of around $0.9$.

\label{tab:pred_res_2080}
\end{table}

\subsection{Inference RTX2080TI}

The resulting $R^2$ scores for our 2080TI inference predictor can be seen in table \ref{tab:pred_res_2080_inf}. For the runtime models we see a steep drop in $R^2$ score from the random forest to the XGBoost model. \\
But even for the better performing random forest model, this is still the worst performing predictor out of our four predictor models. This can be explained by a combination of multiple factors. It is trained on a smaller dataset, since it is not a multi-clock model and it is an inference predictor, which means it has to predict smaller absolute values, which will have larger relative errors in its training set. Lastly, since the 2080TI is a consumer GPU and not a datacenter GPU, its behavior is tuned to focus on the best burst load performance, but not necessarily on the greatest consistency and stability compared to a datacenter GPU such as the A30.


\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
 \textbf{Inference 2080TI}& \textbf{Random Forest} & \textbf{XGBoost} \\
\hline
CV $\overline{R^2}_{time}$ & $0.735 \pm 0.088$ &  $0.607 \pm 0.079$ \\
\hline
Test Set $R^2_{time}$ & $0.77$ & $0.61$ \\
\hline
CV $\overline{R^2}_{power}$ & $0.939 \pm 0.034$  &  $0.944 \pm 0.026$\\
\hline
Test Set $R^2_{power}$ & $0.91$ & $0.86$ \\
\hline
\end{tabular}
\caption{Operations level results for the random forest and XGBoost 2080TI predictors. For the time predictor, the cross-validation prefers the random forest. This is strongly confirmed by the test set scores. The test set scores for the power predictor are not as far apart, but the random forest still performs better.}
\label{tab:pred_res_2080_inf}
\end{table}

% The inference predictor for the 2080TI is our worst performer. It shares the challenges of the 2080TI training predictor, while having to work with a dataset of smaller values, which naturally have larger relative errors. The low scores of the runtime predictors using XGBoost support our decision to use the random forest model going forward. With the random forest predictors, we have a runtime score of $0.73$ and a power score of $0.94$.

\FloatBarrier


\section{Neural Network Level Predictions}


In this next step, we will evaluate the prediction performance of the random forest model for the A30 on the neural network level. \\
% We will perform an more detailed analysis on this part, as this is the abstraction level which is most commonly used and therefore the most relevant to provide predictions for. \\
Since this prediction model is capable of providing predictions for different clock speeds, we will conduct the validation for each clock speed. As this is a graphical validation, we will provide the measured results for each clock speed with the predicted results below. This way both the direct comparison between measurement and prediction is visible, as well as the behavior across different clock speeds. \\
Results are given for both inference and training across all metrics. The covered metrics are runtime, energy and energy delay product, defined in Equation \ref{eq:edp}. The energy delay product serves to illustrate a trade-off between optimizing for either time or energy on their own. \\
Due to the large differences in absolute values, the results for the EDP are normalized per model-input set in order to maintain readability of the plots. \\
\ref{fig:grpfmtimeinf} As expected for the inference runtime measurements, the runtime decreases with an increase in clock speed for all tested models. \\
\ref{fig:gprpredtimeinf} The corresponding inference runtime predictions manage to maintain the ordering between different clock speeds.
% While the absolute results for the predictions may be close, but not perfect, the ordering between different clock speeds is maintained flawlessly from the measurement results. 
This allows the predictions to be used to determine the optimal clock speed. \\
\ref{fig:grpfmenergyinf} As opposed to the monotonous relationship between runtime and clock speed, the optimal clock speed for the inference energy measurements is $900$ MHz. \\
\ref{fig:fprpredenergyinf} As expected from the operations level results, we can see that the predictions for the inference energy are closer to their corresponding measurements than the inference runtime predictions are to theirs. They also maintain the clock speed optimum found in the measurements, making these predictions suitable for energy optimizations. \\
\ref{fig:grpfmtimetrain} For the training runtime measurements we observe the same behavior we saw in the inference case. Lower clock speeds lead to longer runtimes.\\
\ref{fig:grppredenergytrain} In the training runtime predictions, we see a similar prediction performance to the inference case. The predictions provide the correct ordering in this case too.\\
\ref{fig:grpfmenergytrain} Identically to the inference case, the optimal clock speed in terms of training energy measurement lies at 900 MHz. The absolute energy costs for training are between 2 and 5 times as high as the inference costs. \\
\ref{fig:grppredenergytrain} Comparing these training energy predictions to the corresponding measurements just above illustrates how capable our predictions are at reproducing the real world behavior. These predictions are the most precise ones among the presented cases. The ordering is maintained in this case as well.\\
\ref{fig:grppdpfminf} Different from the 900 MHz optimum for energy, we find the optimum for the inference measurement of the energy delay product at 1200 MHz. Being the product of runtime and energy it provides a more balanced metric to optimize along. \\
\ref{fig:grppdppredinf} For the inference predictions of the energy delay product, we find larger discrepancies between measurement and prediction, than for the energy predictions. From the operations level evaluation, we know the power predictions perform better than the time predictions. The fact that time contributes to the energy delay product both as a factor in the energy and then again being multiplied with the energy prediction to form the EDP explains the larger discrepancies. \\
% between measurement and prediction here, than for the energy results. \\
\ref{fig:grppdpfmtrain} For the training measurements of the energy delay product, there is not a lot of change to be observed compared to the inference measurement results in the normalized plot. This indicates a roughly proportional scaling of the energy delay product between inference and training. \\
\ref{fig:grppdppredtrain} In the training predictions of the energy delay product, we see much larger deviations between measurement and predictions compared to the inference case. Runtime predictions are less precise than power predictions and absolute runtimes are several times larger for training than for inference. This effect is amplified by the squared contribution of time towards the EDP. This squared contribution of a larger absolute value which is less precise is a possible explanation of the larger deviations.


% Results for the EDP are normalized for each model-input set, because the orders of magnitude between the different ones are to large for these plots to remain readable otherwise. \\

% Results are given for both runtime and energy, as well as for both inference and training. Additionally, plots of the energy delay product, defined in Equation \ref{eq:edp}, are provided to illustrate a trade-off between optimizing for either time or energy on their own. For the EDP, the results for each model are normalized, because the orders of magnitude between the different models are to large for this plot to remain readable otherwise. \\
% Furthermore, not as a preplanned point of interest for this work, but rather as a fortunate byproduct of the necessary measurements we have also found some patterns in the behavior across clocks, which we will discuss for too.


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Time per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_time_across_clocks_inference.pdf}}
    \caption{Inference time measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmtimeinf}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Time per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_time_across_clocks_inference.pdf}}
    \caption{Inference time predictions for 18 model-input sets across six clock speeds.}
    \label{fig:gprpredtimeinf}
\end{figure}


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Energy per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_energy_across_clocks_inference.pdf}}
    \caption{Inference energy measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmenergyinf}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Energy per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_energy_across_clocks_inference.pdf}}
    \caption{Inference energy predictions for 18 model-input sets across six clock speeds.}
    \label{fig:fprpredenergyinf}
\end{figure}


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Time per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_time_across_clocks_training.pdf}}
    \caption{Training time measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmtimetrain}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Time per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_time_across_clocks_training.pdf}}
    \caption{Training time predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppredtimetrain}
\end{figure}


\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Energy per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_energy_across_clocks_training.pdf}}
    \caption{Training energy measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grpfmenergytrain}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Energy per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_energy_across_clocks_training.pdf}}
    \caption{Training energy predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppredenergytrain}
\end{figure}



\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Normalized Energy Delay Product per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_pdp_inference.pdf}}
    \caption{Inference energy delay product measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grppdpfminf}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Normalized Energy Delay Product per Model Inference}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_pdp_inference.pdf}}
    \caption{Inference energy delay product predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppdppredinf}
\end{figure}



\begin{figure}[htbp]
    \centering
    \parbox{1.1\textwidth}{\centering\textbf{Measured Normalized Energy Delay Product per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/measured_pdp_training.pdf}}
    \caption{Training energy delay product measurements for 18 model-input sets across six clock speeds.}
    \label{fig:grppdpfmtrain}
    \vspace{0.2cm}
    \parbox{1.1\textwidth}{\centering\textbf{Predicted Normalized Energy Delay Product per Model Training}}
    \makebox[\textwidth]{\includegraphics[width=1.1\textwidth]{Plots/clocks/predicted_pdp_training.pdf}}
    \caption{Training energy delay product predictions for 18 model-input sets across six clock speeds.}
    \label{fig:grppdppredtrain}
\end{figure}

\FloatBarrier

\subsection{Patterns and Observations}
Given all our profiling data on the A30, we studied the results in search for the emergence of interesting patterns. \\
We were not surprised to find higher clock speeds resulting in lower runtimes, but apart from this obvious causality we did not find any other curious patterns in our execution time profiling. \\
We did however find an interesting pattern in our energy profiling. Both for the training and the inference case we observed a "U" pattern across the clock speeds. Figure \ref{fig:pattern} shows very high energy costs for very low clock speeds and high energy costs for very high clock speeds. We find the energy minimum at a more moderate clock speed of $900$ MHz. This pattern remains stable across both training and inference and for all tested model-input sets. \\
The fact that the highest clock speeds are not the most efficient choice is explained by the power consumption formula for transistors \cite{hennessy2017computer}. 
\begin{equation}
    P \propto \frac{1}{2} C V^2 f
\end{equation}
Where $P$ is the power, $C$ is the capacitative load, $V$ is the voltage and $f$ is the frequency. \\
For a whole GPU this is scaled by its number of transistors, but the proportionality remains intact. In order to maintain stability at the highest clock speed settings, it is necessary to increase the voltage. The squared contribution of the voltage towards the power consumption makes it very sensitive to larger voltages. This is the reason why the highest achievable clock speed is rarely the most efficient one. \\
We do however observe high energy costs for low clock speeds as well. This is where another effect comes into play. Every GPU has an idle power consumption that is independent of the current workload. At very low clock speeds, a GPU takes much longer to complete its task, resulting in a larger share of the energy usage being caused by its idle power. \\
These two counteracting effects result in the energy optimum lying at neither extreme. So even if a future DNN had a slightly different optimum, the optimum we found here would still be a good starting point when optimizing for energy efficiency.\\
While energy efficiency favors moderate clock speeds, in practice we often require a different balance between energy and runtime. The energy delay product \ref{eq:edp}, quantifies this tradeoff by squaring the time contribution. Optimizing for the EDP shifts our optimum towards higher clock speeds. In Figure \ref{fig:grppdpfminf} and Figure \ref{fig:grppdpfmtrain} we observe a similar "U" pattern in our EDP profiling results, showing an optimum at $1200$ MHz across both inference and training.


\newpage


The shift of the optimum towards higher clock speeds is explained by the definition of the EDP: 
\begin{equation}\label{eq:edp}
    \text{Energy Delay Product} = P \cdot t^2
\end{equation}
Where $P$ is the power and $t$ is the time. \\
The optimum is still the result of the same two counteracting effects. However, since the contribution of the time is squared, the optimum moves to a higher clock speed. \\
Our predictions do not reproduce this pattern perfectly for the training case. Since the runtime has a squared contribution to the EDP and the absolute values for training are larger, this combination leads to a scenario where our predictor's precision for the runtime becomes insufficient to predict the correct optimum. 


% This means that because a GPU running at a very low clock speed will take much longer to complete a workload, a larger fraction of its energy consumption is caused by its idle power.

% This results in a larger fraction of the energy usage being caused by idle power consumption at lower clock speeds, since the lower clock speeds causes longer execution times. 

% This means that because a GPU running a workload at a very low clock speed will take a long time to complete it, a larger fraction of the energy cost will be caused by its idle power consumption. \\

% At a more moderate clock speed of $900$ MHz, we find an energy minimum. 

% In between, for moderately high clock speeds, we find a minimum for the required energy at a clock speed of $900$ MHz.


% While we are not surprised by the runtime measurement results simply exhibiting shorter runtimes for higher clock speeds, we do find an interesting pattern for the energy measurement results. 

% When trying to optimize for a balance between energy and runtime, the optimum shifts towards a higher clock speed. Looking at the energy delay product,

\begin{figure}[htbp]
    \centering
    \parbox{0.7\textwidth}{\centering\textbf{Energy Cost "U" Pattern}}
    \makebox[\textwidth]{\includegraphics[width=0.7\textwidth]{Plots/example_model_plot.pdf}}
    \caption{Energy measurements across different clock speeds for a single model-input set. The "U" pattern which is present for all model-input sets becomes even more apparent. In terms of energy cost it is most efficient to run both inference and training tasks at a clock speed of $900$ MHz for all our tested model-input sets.}
    \label{fig:pattern}
\end{figure}

\FloatBarrier



\section{Predictor Latency}

Other than the predictor performance in terms of accuracy, its performance in terms of latency is also a concern. \\
The loading of the dataset turned out to be a very time intensive operation. Even after improving the performance, we arrived at a loading time of around 30 seconds for the full set of model-input sets used to validate the predictor. These 30 seconds are in a completely different order of magnitude in terms of execution time compared to their preprocessing, their prediction or even their execution. Realistically, this time is more comparable to having to download a model before being able to benchmark it. \\
The next largest contributor in terms of execution time is preprocessing the dataset in order to format is in a way that can be used as an input for our predictor models. In its current form this step takes somewhere around 500 ms. For the full validation set of 18 model-input sets it takes around 600 ms. For an individual model it takes less than 500 ms. \\
The most important evaluation regarding predictor latency is the execution time of the actual predictor model. Predicting time and energy at a specific clock speed for all model-input sets in the validation set demonstrates very good performance. Across multiple clock speeds and for both for the training predictor and the inference predictor the resulting execution time hovers around 30 ms. Even for the scenario with the smallest execution time, inference at maximum clock speed, this is still faster than the validation set's collective execution time of around 300 ms by one order of magnitude. This comparison shifts only further in favor of the predictor model when we look at a still very reasonable scenario of training at a clock speed of 930 MHz. With this, the collective execution time is close to 2000 ms, while the prediction time is not affected. Here we have a predictor latency over 60 times lower than performing the measurement instead.