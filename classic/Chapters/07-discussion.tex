\chapter{Discussion and Outlook}\label{chap:discussion}

\section{Discussion}

The approach we took in this work was of an exploratory nature. Rather than diving deep into one specific configuration, we chose to go wide and look at multiple hardware and many, many neural network scenarios. That approach turned out to be both blessing and curse. A blessing, because bare any limitations, the results would be applicable in a very wide field of applications and our findings are understandable at a more abstract level, which does not require a detailed understanding of subject matter. A curse, because we have to hamper our scientific curiosity not to follow every rabbit hole we encounter on our journey. Given the wide approach, we also encounter a wide range is troubleshooting issues, of various importance to the study itself. This balancing act of weighing the cost and benefit of following up interesting trends and deciding whether to invest the time to overcome obstacles was the most difficult part of this work. In the same way in which there is no end in going deep, no state of completeness, there is also no completeness to be achieved in going wide. And while, from an approriate distance, this is obvious, it is also fundamentally frustrating to deal with.\\
In order to go wide, one would like to include a large number of hardware scenarios and study all of them with the same set of accurate tools. In an ideal setting, we would include GPUs from Nvidia, AMD, Intel and further FPGAs and other ML accelerating coprocessors such as IPUs. But the platforms and tools are to varied, that even if we found a tool which was able to measure power for many of them, we would likely give up both time and power resolution in exchange for the improved compatibility. Another simple but very important limitation is the question of the hardware available to us. Because of these considerations as well as in order to keep the scope of this thesis in check, we decided to limit ourselves to Nvidia GPUs. \\
Another dimension to go wide in is the plurality of neural networks to include in our study. A central contribution to that decision was our platform of choice. Due to prior experience with the platform we chose PyTorch. In order to use well known networks and implementations, we chose to work with neural networks from the torchvision library. This also improves the easy of reproducing our study. However, our GPU with the smallest amount of global memory added a limit to which model-input sets we were able to include in our suite of model-input sets profiled for the training set. Each model-input set had to complete all benchmarks on all hardware configurations. In a few cases a model-input set worked on the A30 but not on the 2080TI. The reason was not apparent in every case, and while in a larger work it might have been possible to determine it, this would have gone beyond the scope we were able to maintain here. \\
Our choices to study both time and energy for NN inference and training were shaped by a desire to study a novel section of this field, adding value to our research.  \\
After initial attempts to include the clock speed study for both GPUs, after running into issues on the 2080TI, it was dropped from this part of the overall study.



\section{Outlook}

\section{Conclusion}
