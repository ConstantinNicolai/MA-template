\chapter{Discussion and Outlook}\label{chap:discussion}

\section{Discussion}

The approach we took in this work was of an exploratory nature. Rather than diving deep into one specific configuration, we chose to go wide and look at multiple hardware and many neural network scenarios. That approach turned out to be both a blessing and a curse. A blessing, because the results can be used in a wide field of applications and our findings are understandable at a more abstract level. A curse, because we have to hamper our scientific curiosity not to follow every rabbit hole we encounter along our journey. Given the wide approach, we also discover a wide range is troubleshooting issues which vary in their importance to the study itself. This balancing act of weighing the cost and benefit of following up on interesting trends and deciding whether or not to invest the time to overcome the troubleshooting obstacles was the most difficult part of this work. In order to understand the choices we made concerning the scope, it is important to see that even in our quest to go wide, it lies in the nature of exploratory research, that no state of completeness can ever be achieved. \\
%In the same way in which there is no end in going deep, no state of completeness, there is also no completeness to be achieved in going wide. And while, from an appropriate distance, this is obvious, it is also fundamentally frustrating to deal with.
In order to go wide, one would like to include a large number of hardware scenarios and study all of them with the same set of accurate tools. Ideally, we would like to include GPUs from Nvidia, AMD and Intel alongside FPGAs and other ML accelerators. But the platforms and tools are too varied, such that even if we found tooling which was capable of measuring power for all of the above, we would likely give up both time and power resolution in exchange for that improved compatibility. Another simple but very important limitation is the hardware available to us. Both because of these considerations as well as in order to keep the scope of this thesis in check, we decided to limit ourselves to Nvidia GPUs. \\
Another dimension to go wide in, is the plurality of neural networks. Here our decision was determined by our platform of choice. Due to prior experience with the platform we chose PyTorch. In order to use well known networks and implementations, we decided to work with neural networks from the torchvision library. This also improves the easy of reproducing our study. Our GPU with the smallest amount of global memory added a limit to which model-input sets we were able to include in our suite profiled for the training set, since each model-input set had to be able to complete all benchmarks on all hardware configurations. \\
%In a few cases a model-input set worked on the A30 but not on the 2080TI. The reason was not apparent in every case, and while in a larger work it might have been possible to determine it, this would have gone beyond the scope we were able to maintain here. \\
Our choice to study both time and energy for inference and training was shaped by the desire to study a novel section of the field and to add value to our research.  \\
After initial attempts to include both GPUs in the clock speed study, the 2080TI was dropped from this part of the overall study after running into issues.\\
% All of this shapes into our scope. A scope which was not predetermined from the get go. Rather it evolved hand in hand with the study. In an earlier phase of the study, the main validation of our measurement methodology took place. At that time, we had not yet began setting the clock speed. Instead we worked with both the 2080TI's and the A30's default configuration and added on top, the A30, but with disabled tensor cores. The same is true for the addition of training into study. However the identical methodology and the resulting accuracy of the training predictors should suffice to scatter any remaining doubts concerning the dataset of training operations.\\
% Even though the predictor models for the A30 and the RTX2080TI differ in the former including clock speed specific predictions, they otherwise work the the exact same fashion. 
The predictor models for the A30 and the 2080TI only differ in the A30 predictor including clock speed specific predictions. Apart from that they are built identically. In our evaluation of which type of predictor model we want to use, we are therefore looking for the model type that is the most stable across all our scenarios. This will enable it to serve as a recommendation suitable for various settings of neural networks and hardware. With that in mind, let us recap our predictor results. For the 2080TI, the evaluation of the $R^2$ score showed similar or better performance using the random forest model over the XGBoost model for both the training and inference predictors. The evaluation of the A30 predictor revealed slightly better scores using the XGBoost predictor. However, the improvements over the random forest model were much smaller than the drop-off for the 2080TI predictor going from random forest to XGBoost. This behavior leads us to believe that the random forest approach serves as a more stable and consistent path across the plurality of hardware configurations this methodology might encounter in the future. \\
Earlier in this discussion it was mentioned that this is an exploratory work. In the same way it is also a foundational work. Each contribution aids to explore which paths can be taken and helps building a foundation of methodology and due diligence. Our work provides the tools for the collection of a required dataset and demonstrates the resilience of the resulting dataset through a direct validation. Since the predominant interest in the research community lies in finding insights regarding complete neural networks, the nature of our validation ensures our findings are applicable to full neural network executions. \\ 
For the same reasons, the A30 predictor is evaluated on a neural network level. Operations level results might be interesting for a specific academic niche, but they can only fulfil their potential when they are applicable to full neural networks. \\
While the primary reason for these evaluations and validations is to establish the capabilities and limitations of the specific predictors trained in this work, they also serve to establish the soundness of the approach and methodology on a broader level, demonstrating their suitability as a base for future work.


% Through that it shows that this approach is appropriate for the common area of interest of full neural networks. It explores prediction models across different GPUs and granularities of parameters. The predictors are evaluated on each GPU on the operations level, investigating $R^2$ score performance for multiple predictor model options. On top of this, to evaluate the predictor of the A30 on the full neural network level, a test set of model-input sets is introduced and time, energy and energy delay product are presented in a graphical evaluation across a set of clock speeds. While these evaluations and validations obviously serve to establish the capabilities and limitations of the specific predictors trained here in this work, on a broader level they serve to establish the soundness of the approach and methodology providing them as a base for future work.

% PREDICTOR MODEL OPTIONS, RESULTING ACCURACY LEADS TO choice

% FOUNDATIONAL WORK, OPENING THE DOOR TO  BUILDING BROADER DATASETS WITH THE BASIC DUE DILLIGENCE FOR THE METHODOLOGY ALREADY DONE 

% IN THE CURRENT STATE ALREADY USEFUL AND APPLICABLE FOR SOME CASES BUT MAINLY A BASIS TO BUILD UPON AND EXPAND --> OUTLOOK



% WHAT ARE OUR TAKEAWAYS FROM THE RESULTS IN CONTRIB 3?\\
% A30 PREDICTOR IS MORE ACCURATE, MAYBE DUE TO LARGER TRAINING SET FROM MULTI CLOCK APPROACH. XGBOOST APPEARS TO SCALE TO BETTER ACCURACY WITH THIS LARGER DATASET  

% 2080TI PREDICTOR SHOWS WORSE PERFORMANCE. MIGHT BE DUE TO SMALLER DATASET AND LESS CONSISTENT BEHAVIOUR OF A CONSUMER GPU



% THIS COVERS WHY WE TOOK THE PART OF THE MAP WE DID. NOW WE DISCUSS PREDICTION MODELS SHORTLY AND TRANSITION INTO VALIDATION AND ITS PURPOSE HERE THEN FOLLOWING UP WITH THIS WORKS APPLICATION. THIS SHOUD LEAD INTO THE OUTLOOK



\section{Outlook}
% As already established in the discussion, this is an exploratory and foundational work in its nature, leading to a very broad possible outlook. For this reason, we will attempt to focus on most promising and clear avenues for future work we encountered in our study. \\
The nature of this work opens up a number of avenues for future work. \\
In the light of wanting to guide towards use of the best fitting hardware for a specific task and requirements, the most promising avenue would be to expand the study to a larger number of hardware platforms. The lowest friction way of doing that would be to collect datasets for further Nvidia GPUs at their default clock and add the GPU model as a parameter to the prediction model, in the same way the clock speed was added for the A30 predictor. \\
Another direction could be an attempt to improve the prediction performance.
A good point to start would be the expansion of the set of model-input sets which were used to build the training set for the predictors in this work. Including operations from more model-input sets, especially types of models not included in this work, could go a long way towards improving the predictor's accuracy and generalizability beyond its current state. \\
Even though we limited our GPU clock study to the A30 in this work, there is no conceptual reason, why these kinds of clock speed studies could not be expanded to a more GPUs, as long as they support setting the clock speed manually. This kind of study always carries the chance of discovering interesting patterns and behaviors.\\
Another davenue for expansion are the metrics included in the study. The metrics used in our work were fixed from a very early point onward. It covers time, power and energy. A prime candidate for the next metric to add would be memory usage. The complexity in the addition of further metrics lies in the tooling. The more tools are used, the harder expansion towards additional hardware platforms becomes. Additionally, depending on the types of hardware added, not all metrics which are meaningful for a GPU are necessarily meaningful for all hardware types. \\
The last and widest avenue for expansion is the inclusion of more hardware platforms. Our current tooling for power readout is Nvidia specific, but provided equivalent tools, an expansion towards GPUs from different manufacturers like AMD and Intel would be very interesting. Combined with real-time pricing this would allow determining the most cost effective GPU for a specific task including power costs across the entire GPU market. The last and most difficult step is moving from GPU studies towards the inclusion of CPUs and more exotic accelerators like FPGAs and IPUs. It is this step which will be the most limiting to our selection of suitable metrics, since they need to be meaningfully applicable to all included hardware platforms. \\
Expanding the number and types of hardware platforms is the most fascinating avenue for future work, but it is also the one moving the furthest away from the foundation presented in this work and the least predictable in its development. \\
In summary, there are many opportunities for future expansion upon the basis presented here. Some of them will require a lot of additional work, while others are direct continuations, which should present minimal friction.

\section{Conclusion}


% \begin{itemize}

% \item how we adressed our stated research gap with that

% \item short rehash of contribs


% \item we have a usable tool as is


% \item both the predictions as well as the profiled measurements themselves can provide useful insights

% \item it can serve as a foundation for future work



% \item the available research potential is far from exhausted


% \end{itemize}

We identified the research gap as the lack of works covering performance predictions for runtime and energy for both training and inference workloads. In order to address this, we designed our profiling and prediction frameworks to cover time, power and energy for both inference and training operations. \\
Our first contribution covers the profiling part of this work. Here we presented our method to collect a dataset of individual operations which serves as training data for our predictor. The second contribution presents the predictor and our choice of a random forest model, as well as the preprocessing of the dataset. Our third and largest contribution is focused on validating and evaluating the work product of the first two contributions. It contains a validation of the dataset quality through comparisons between direct measurements of full neural network runs and summed up operations-level profiling results. The random forest predictor models are evaluated via $R^2$ score with both cross-validation and test set performance on the operations level. On the neural network level, a set of unknown model-input sets is used to validate the predictor performance by comparing direct measurements of the full neural network runs to the summed up operations-level predictions. The necessary profiling over a number of clock speeds also yielded the insight, that 900 MHz is the most energy efficient setting on the A30 across all model-input sets used for the validation. \\
With $R^2$ scores of $0.7$ to $0.9$ for runtime predictions and $0.90$ to $0.98$ for power predictions on the operations level, we already contribute a tool which can be of considerable help in deciding which GPU and which clock speed to run a specific model at. Additionally the raw profiling data itself bears the potential of providing insight into useful patterns like the most energy efficient clock speed or the optimal clock setting for a minimal energy delay product. \\
Our work serves as an exploratory and foundational step into the profiling and prediction of an ever broadening zoo of parameter combinations between models, inputs, hardware platforms, clock settings, metrics and workload types. And while we contribute valuable findings and tools towards that aim, the available research potential in this field is far from exhausted and we hope our findings and contributions can help to pave the way and guide future researchers in search of that same goal.