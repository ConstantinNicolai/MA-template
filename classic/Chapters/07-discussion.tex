\chapter{Discussion and Outlook}\label{chap:discussion}

\section{Discussion}

The approach we took in this work was of an exploratory nature. Rather than diving deep into one specific configuration, we chose to go wide and look at multiple hardware and many, many neural network scenarios. That approach turned out to be both blessing and curse. A blessing, because bare any limitations, the results would be applicable in a very wide field of applications and our findings are understandable at a more abstract level, which does not require a detailed understanding of subject matter. A curse, because we have to hamper our scientific curiosity not to follow every rabbit hole we encounter on our journey. Given the wide approach, we also encounter a wide range is troubleshooting issues, of various importance to the study itself. This balancing act of weighing the cost and benefit of following up interesting trends and deciding whether to invest the time to overcome obstacles was the most difficult part of this work. In the same way in which there is no end in going deep, no state of completeness, there is also no completeness to be achieved in going wide. And while, from an approriate distance, this is obvious, it is also fundamentally frustrating to deal with.\\
In order to go wide, one would like to include a large number of hardware scenarios and study all of them with the same set of accurate tools. In an ideal setting, we would include GPUs from Nvidia, AMD, Intel and further FPGAs and other ML accelerating coprocessors such as IPUs. But the platforms and tools are to varied, that even if we found a tool which was able to measure power for many of them, we would likely give up both time and power resolution in exchange for the improved compatibility. Another simple but very important limitation is the question of the hardware available to us. Because of these considerations as well as in order to keep the scope of this thesis in check, we decided to limit ourselves to Nvidia GPUs. \\
Another dimension to go wide in is the plurality of neural networks to include in our study. A central contribution to that decision was our platform of choice. Due to prior experience with the platform we chose PyTorch. In order to use well known networks and implementations, we chose to work with neural networks from the torchvision library. This also improves the easy of reproducing our study. However, our GPU with the smallest amount of global memory added a limit to which model-input sets we were able to include in our suite of model-input sets profiled for the training set. Each model-input set had to complete all benchmarks on all hardware configurations. In a few cases a model-input set worked on the A30 but not on the 2080TI. The reason was not apparent in every case, and while in a larger work it might have been possible to determine it, this would have gone beyond the scope we were able to maintain here. \\
Our choices to study both time and energy for NN inference and training were shaped by a desire to study a novel section of this field, adding value to our research.  \\
After initial attempts to include the clock speed study for both GPUs, after running into issues on the 2080TI, it was dropped from this part of the overall study.\\
All of this shapes into our scope. A scope which was not predetermined from the get go. Rather it evolved hand in hand with the study. In an earlier phase of the study, the main validation of our measurement methodology took place. At that time, we had not yet began setting the clock speed. Instead we worked with both the 2080TI's and the A30's default configuration and added on top, the A30, but with disabled tensor cores. The same is true for the addition of training into study. However the identical methodology and the resulting accuracy of the training predictors should suffice to scatter any remaining doubts concerning the dataset of training operations.\\
Even though the predictor models for the A30 and the RTX2080TI differ in the former including clock speed specific predictions, they otherwise work the the exact same fashion. In our evaluation of which type of predictor model we want to use, we therefore are looking for a model type that is stable across more than one scenario. If that is given, it can serve as a recommendation for different settings, be it on the same hardware or different a different one. With that in mind, let us recap our predictor results. For the RTX2080TI our predictor is limited to the default clock, since the per clock study was omitted for this GPU. The evaluation of the $R^2$ score with a 15 fold cross-validation as well as the direct score of the test set showed similar or better performance using the random forest model over the XGBoost model for both the training and the inference predictor. The evaluation of the A30 predictor revealed slightly better scores using the XGBoost predictor across the bord. However, the improvements over the random forest model were much smaller than the dropoff the other way around with the RTX2080TI predictor. This behavior leads us to believe that the random forest approach serves as a more stable and consistent path across the plurality of hardware configurations this methodology might be used on in the future. \\
Earlier in this discussion, it was mentioned, that this is an exploratory work. This is true, but is the same way it is also a foundational work. Each contribution in this work aids to explore which paths can be taken and builds a foundation of methodology and due dilligence for our general approach. It provides the tools for the collection of a required dataset and demonstrates the resilience of the resulting dataset through a direct validation in the full neural network setting. Through that it shows that this approach is approriate for the common area of interest of full neural networks. It explores prediction models across different GPUs and granularities of parameters. The predictors are evaluated on each GPU on the operations level, investigating $R^2$ score performance for multiple predictor model options. On top of this, to evaluate the predictor of the A30 on the full neural network level, a test set of model-input sets is introduced and time, energy and energy delay product are presented in a graphical evaluation across a set of clock speeds. While these evaluations and validations obsiously serve to establish the capabilities and limitations of the specific predictors trained here in this work, on a broader level they serve to establish the soundness of the approach and methodology providing them as a base for future work.

% PREDICTOR MODEL OPTIONS, RESULTING ACCURACY LEADS TO choice

% FOUNDATIONAL WORK, OPENING THE DOOR TO  BUILDING BROADER DATASETS WITH THE BASIC DUE DILLIGENCE FOR THE METHODOLOGY ALREADY DONE 

% IN THE CURRENT STATE ALREADY USEFUL AND APPLICABLE FOR SOME CASES BUT MAINLY A BASIS TO BUILD UPON AND EXPAND --> OUTLOOK



% WHAT ARE OUR TAKEAWAYS FROM THE RESULTS IN CONTRIB 3?\\
% A30 PREDICTOR IS MORE ACCURATE, MAYBE DUE TO LARGER TRAINING SET FROM MULTI CLOCK APPROACH. XGBOOST APPEARS TO SCALE TO BETTER ACCURACY WITH THIS LARGER DATASET  

% 2080TI PREDICTOR SHOWS WORSE PERFORMANCE. MIGHT BE DUE TO SMALLER DATASET AND LESS CONSISTENT BEHAVIOUR OF A CONSUMER GPU



% THIS COVERS WHY WE TOOK THE PART OF THE MAP WE DID. NOW WE DISCUSS PREDICTION MODELS SHORTLY AND TRANSITION INTO VALIDATION AND ITS PURPOSE HERE THEN FOLLOWING UP WITH THIS WORKS APPLICATION. THIS SHOUD LEAD INTO THE OUTLOOK



\section{Outlook}
As already established in the discussion, this is an exploratory and foundational work in its nature, leading to a very broad possible outlook. For this reason, we will attempt to focus on most promosing and clear avenues for future work we encountered in our study. \\
In the light of wanting to guide towards use of the best fitting hardware for a spefic task and requirements, the most promising avenue for future work would be to expand the study to a larger number of hardware platforms. The lowest friction way of doing that would be to collect datasets for further Nvidia GPUs at their default clock and add the GPU model as a paramter for the prediction model, in the same way, the clock speed was added to the A30 predictor. \\
Another direction could be an attempt at improving the prediction performance. A good starting point in order to do that could be to expand the base set of model-input sets which was used in this work to build the training set for the predictor. Including operations from even more model-input sets, maybe even types of models not included in this work could go a long way towards improving the accuracy and generalizability beyond its current state. \\
Even though we limited our GPU clock study to the A30 in this work, there is no conceptual reason, why these kinds of clock speed studies could not be expanded to a more GPUs, as long as they support setting the clock speed manually. This might not be the most novel contribution of all time, but it does carry the chance of discovering interesting patterns of behaviors.\\
Another dimension which could be expanded is one which was fixed from a very early point in our study onwards. This is the dimensions of which metrics are included in the study. In our study we covered time, power and energy. Conceptually, it is a very simple step to add further metrics and can provide a lot of additional insight. The difficult part in this is the question of tooling. If tooling collecting measurements of an additional metric is found, which can be added into the dataset collection without major complications, this is very promising. A prime candidate for the next metric to add would be memory usage. However, the more different tools are used, the harder expansion towards different GPUs, or even other hardware platforms will become. Additionally, depending on the type of hardware platforms one might want to add in further studies, not all metrics which can be applied to GPUs are necessarily meaningful for another platform. 


4 different GPU vendors like AMD and Intel


5 going very far away from the presented methodolgy including different types of platforms like CPU and other accelerators like IPUs and FPGAs. 




\section{Conclusion}
