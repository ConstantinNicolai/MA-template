\chapter{Introduction}\label{chap:introduction}

\section{Motivation}

The global increase in usage of machine learning applications
 illustrates an acceleration in adoption across
both industry and the private sector. The unfathomably
large energy costs tied to this broader adoption have
already prompted a change in public sentiment towards
energy infrastructure. Plans for building
trillion-dollar data centers are emerging, necessitating
the re-commissioning of previously decommissioned
nuclear power plants, which were originally phased
out as part of nuclear energy reduction efforts. This
reversal of nuclear phase-out policies underscores the
significant infrastructural and political pressures
exerted by the energy requirements of machine learning
technologies. \\
In this landscape it is more pressing than ever to gain
insight into the roots of the energy costs in order to
optimize future developments on an informed basis. \\
In order to facilitate a more informed pairing of workload 
and GPU we introduce a framework to help guide the decision 
towards an optimal choice. By allowing for optimization towards the fastest execution time or the smallest energy footprint, our framework enables an informed choice which prevents wasteful computation by optimizing for energy when execution time is not critical. 


\section{Problem Statement}
While a considerable amount of previous work has been done in
profiling and prediction of neural network performance, no prior work covers both execution time and power consumption across training and inference.\\
However in most cases where a new model architecture is designed or an existing architecture is adapted, both the training and the inference efficiency are relevant at some point in the model's lifespan. As training is typically performed in datacenters, it can be both time-constrained by long execution times and strict deadlines, as well as energy-constrained by infrastructure limitations in cooling capabilities and energy budget. Inference on the other hand can be latency-constrained by real-time applications but also energy-constrained on mobile or embedded devices. 

\section{Scope}

We focus our work exclusively on the profiling and prediction of deep neural networks. \\
The intended scope is to collect a dataset, validate its usability, use it to perform predictions and evaluate the prediction quality. \\
Practical constraints narrowed our study to the Nvidia RTX 2080 TI and the A30 GPUs and to the models available in the Pytorch Torchvision library\footnote{\url{https://docs.pytorch.org/vision/0.22/}}. \\
The grid-search approach across the parameters revealed some inherent incompatibilities, such as models too large to fit the GPU memory or benchmarks resulting in prohibitively long runtimes due to edge case parameters combinations. Rather than troubleshooting every edge case, we decided to prioritize more practical configurations and to exclude those edge cases of limited practical relevance.

\section{Contributions Overview}
\subsection{Dataset Collection}

The first contribution introduces a method for automated profiling data collection and processing into a coherent dataset.\\
In order to ensure repeatability, the time and power readings along with a number of key related metrics are stored together with the Pytorch objects of the profiled operations. \\
The profiling measurements are executed via a python script that utilizes the Pytorch Benchmark library\footnote{\url{https://pytorch.org/tutorials/recipes/recipes/benchmark.html}} to conduct the time measurements. The power profiling is conducted by running nvidia-smi\footnote{\url{https://docs.nvidia.com/deploy/nvidia-smi/index.html}} in the background during the benchmark execution. 

\subsection{Prediction Model}
The second contribution presents our prediction model. By aggregating the predictions for the individual operations of a neural network, we can infer predictions for the complete DNN. This allows us to identify which GPU will have the shortest execution time and which one the smallest energy footprint for any given DNN architecture. 

\subsection{Validation}
The third and final contribution consists of two major sections.\\
The first one investigates the quality of our collected dataset of individual operations. The compositional validity of the dataset is confirmed by comparing the aggregation of the individual operations which compose a neural network against measurements of the full neural network. \\
The second section assesses the performance of our prediction model both for individual operations and for full neural networks.