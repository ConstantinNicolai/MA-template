\chapter{Introduction and Motivation}\label{chap:introduction}

\section{Motivation}

The global increase in usage of machine learning applications
 illustrates an acceleration in adoption across
both industry and the private sector. The unfathomably
large energy costs tied to this broader adoption have
already prompted a change in public sentiment towards
 energy energy infrastructure. Plans for building
trillion-dollar data centers are emerging, necessitating
the re-commissioning of previously decommissioned
nuclear power plants, which were originally phased
out as part of nuclear energy reduction efforts. This
reversal of nuclear phase-out policies underscores the
significant infrastructural and political pressures
exerted by the energy requirements of machine learning
technologies. \\
In this landscape it is more pressing than ever to gain
insight into the roots of the energy costs in order to
optimize future developments on an informed basis. \\
In oder to facilitate a more informed pairing of workload 
and GPU we introduce a framework to help guide the decision 
towards an optimimal choice. This way regardless whether 
the fastest execution or the smallest energy footprint is 
desired, the informed choice enabled by our framework
prevents wasteful computation.
% Therefore we are taking a closer look at the making
% up of these omnipresent machine learning models and
% will perform a quantitative study of the operations that
% they are made up from. 

\section{Problem Statement}
While a considerable amount of previous work has been done in
profiling and prediction of neural network performance, no prior
work covers the same cases and the same performance metrics.
Therfore, our study investigates both training and inference cases 
covering both execution time and power consumption. \\
This contribution is valuable because in most cases where a new model
architecture is designed or an existing architecture is adapted, 
both the training and the inference efficiency are relevant at some
point of the models lifespan. At the same time latency or power 
envelope requirements may be fluent between the training and the
inference stage, necessitating both performance metrics.

Introduction to your topic and motivation of your work.
Example citation \cite{bishopPatternRecognitionMachine2006} (good book!).
Table~\ref{tab:example} shows an example table and Figure~\ref{fig:example} an
example plot.

\input{Tables/example}

\input{Plots/example}

