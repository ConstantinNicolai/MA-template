\chapter{Introduction}\label{chap:introduction}

\section{Motivation}

The global increase in usage of machine learning applications
 illustrates an acceleration in adoption across
both industry and the private sector. The unfathomably
large energy costs tied to this broader adoption have
already prompted a change in public sentiment towards
 energy energy infrastructure. Plans for building
trillion-dollar data centers are emerging, necessitating
the re-commissioning of previously decommissioned
nuclear power plants, which were originally phased
out as part of nuclear energy reduction efforts. This
reversal of nuclear phase-out policies underscores the
significant infrastructural and political pressures
exerted by the energy requirements of machine learning
technologies. \\
In this landscape it is more pressing than ever to gain
insight into the roots of the energy costs in order to
optimize future developments on an informed basis. \\
In order to facilitate a more informed pairing of workload 
and GPU we introduce a framework to help guide the decision 
towards an optimal choice. This way regardless whether 
the fastest execution or the smallest energy footprint is 
desired, the informed choice enabled by our framework
prevents wasteful computation.
% Therefore we are taking a closer look at the making
% up of these omnipresent machine learning models and
% will perform a quantitative study of the operations that
% they are made up from. 

\section{Problem Statement}
While a considerable amount of previous work has been done in
profiling and prediction of neural network performance, no prior
work covers the same cases and performance metrics targeted in this work.
Therefore, our study investigates both training and inference cases 
covering both execution time and power consumption. \\
This contribution is valuable because in most cases where a new model
architecture is designed or an existing architecture is adapted, 
both the training and the inference efficiency are relevant at some
point of the models lifespan. At the same time, latency or power 
envelope requirements may be fluent between the training and
inference stages, necessitating the study of both performance metrics.


\section{Scope}
The intended scope for this thesis is collecting a dataset, validating the usability of said dataset, using it to perform predictions and lastly evaluating the quality of the predictions. \\
In practice there are also limitations of the scope in terms of which hardware and which neural networks are included in the study. Here the hardware is limited to the Nvidia RTX 2080 TI and the Nvidia A30. The neural networks in use are all part of the Pytorch Torchvision library. \\
Lastly, due to the grid-search like nature of the study, there are points in the grid where some incompatabilities occurs. This may be a model which does not fit onto a GPU or a benchmark which takes a prohibitive amount of time for a specific setting. Given the amount of troubleshooting necessary to find workarounds for all these edge cases of little practical applicability, we take the freedom of not taking them into the scope of this work.


\section{Contributions Overview}
\subsection{Dataset Collection}
In our first contribution we introduce our method for collecting 
profiling data and creating a coherent dataset by processing the 
collected data.\\
The profiling measurement is executed via a python script, utilizing
the Pytorch Benchmark 
library\footnote{\url{https://pytorch.org/tutorials/recipes/recipes/benchmark.html}}
to conduct the execution time measurements. The power measurements
are conducted using the command-line tool 
nvidia-smi\footnote{\url{https://docs.nvidia.com/deploy/nvidia-smi/index.html}}
being run in the background while the benchmark is performed.\\
In order to enable repeatability, the execution times and power readings
along with their respective measurement errors are stored together with the
Pytorch objects for profiled operations, along with a sensible selection of
related metrics.

\subsection{Prediction Model}
In our second contribution we present our prediction model. Based on the predictions
for the operations occurring in a neural network we can infer a prediction for the 
complete network. We can therefore provide insight into which of the studied GPUs
is most suitable for a given neural network, depending on the metric of interest.
This way we can identify both the GPU which can execute the neural network in the
fastest time, and the GPU which results in the smallest energy consumption and
accompanying heat output.

\subsection{Validation}
In our third and final contribution we investigate the quality of our dataset of
measurements for individual operations by comparing the sum of individual operations
against a full neural network measurement for both execution time and energy consumption.\\
In the second part of this contribution we assess the accuracy of our predictions
both for individual operations as well as for full neural networks. Both for the
individual operations as well as for the full neural networks this is achieved by
comparing the predictions to measurements acquired using the same measurement
methodology used to collect the initial dataset.





% Introduction to your topic and motivation of your work.
% Example citation \cite{bishopPatternRecognitionMachine2006} (good book!).
% Table~\ref{tab:example} shows an example table and Figure~\ref{fig:example} an
% example plot.

% \input{Tables/example}

% \input{Plots/example}

