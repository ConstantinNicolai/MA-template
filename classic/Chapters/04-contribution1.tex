\chapter{Dataset Collection}\label{chap:contrib1}

% This contribution serves to introduce the method used
% to collect a profiling dataset spanning a number of
% parameters. This serves as a training set for the
% prediction model. The dataset spans operations from
% Torchvision models and input sizes, execution time 
% and power, inference and training case as well as
% different GPUs and various GPU clocks.

This contribution outlines the method used to collect a profiling dataset. The  dataset serves as a training set for the prediction model. The parameters covered are various Torchvision models and input sizes, execution time and power, both the inference and the training case as well as different GPUs and various GPU clocks.


\section{Operations}


In order to increase generality our approach exploits the layerwise structure of DNNs. This is achieved by working on the layer level rather than on the model level. In order to be rigorous we need to be clear on the terminology. The workload and its characteristics depend on the layer and its input features, just like it would on the model level on the DNN and the input dimensions. We are interested in most general objects with a the same characteristic workload. On the layer level those will be layers with specific input feature dimensions and layer settings. We will refer to these objects as operations. On the model level those will be DNNs with specific input image dimensions, which we will refer to as model-input sets. \\
The units for which we will later make predictions are individual operations. To this end, we aim to collect a dataset of operations and profile the execution time and wattage for each one. To ensure that the dataset reflects operations encountered in real-world scenarios, we select a number of representative model-input sets from models of the Torchvision library. The set of operations we will be profling, is the set of all operations occuring in any of these model-input sets. \\
Extracting this set is automated via scripts, but can just as well be done by hand if so desired. With this set at hand we can dive into the actual profiling.


\section{Time Profiling}

% import torch.utils.benchmark as benchmark

% timer = benchmark.Timer(
%     stmt="run_inference(operators, num_layers,required_iterations, ifmap)",  # Statement to benchmark  # Setup the function and variables
%     setup="from __main__ import run_inference",
%     globals={
%         "operators": operators,
%         "required_iterations": required_iterations,
%         "num_layers": num_layers,
%         "ifmap": ifmap
%     },
%     num_threads=1,  # Number of threads to use
%     label="Latency Measurement",
%     sub_label="torch.utils.benchmark"
% )

% profile_result = timer.blocked_autorange(callback=None, min_run_time=rundur * runnr)

The time profiling is performed using the benchmark utility from torch.utils.benchmark called Timer. This enables us to run our benchmark function with the specific layer and input features as input parameters, allowing us to run it for each operation. \\
To achieve more stable results and inprove the simultaneous power profiling it is desirable to run the benchmark for each operations for at least a few seconds. This is achieved by setting a minimum runtime for the benchmark to aggregate statistics. Different workload characteristics and different computational capabilities depending on the GPU configuration being profiled result in different requirements for individual profiling runs in order to ensure sufficient repitions in the measurement process. In order to accomodate this requirement, the runtime of the measurement is configurable via a command-line parameter. \\
For the initialization of the operations we are going to benchmark we have to be careful to avoid to extreme cases in order to achieve a sensible approximation of the execution characteristics within a neural network. In the first case we initialize one instance of the operation and perform each benchmark loop on that instance. This will result in hitting the cash everytime and not be representative. On the other end of the spectrum of possibilities we have the case of initializing a new instance for each loop of the benchmark run, which adds the initializition overhead and the larger latency of having to access the GPU main memory every time, skewing our results in that direction. We have therefore chosen the middleground approach of initializing a sizable block of operation instances with random weights and biases, which is looped over by the benchmark kernel, resulting in some reuse of layers, stressing the memory system in a sensible manner. \\


\subsection{Inference}
The central difference between inference and training profiling appears in the design of the benchmark function. \\
For inference profiling we put our operations into \texttt{eval} mode and call the operations within a \texttt{torch.inference\_mode} environment, which is equivalent to the exection in a typical inference forward pass through a model. Within the loop we call the operator on a preinitialized random input tensor for the forward pass. The input tensor is a single instance, as it represents the activations from the previous layer which can be expected to live in high level memory in our memory hierarchy. Since this forward pass is everything we want to profile for inference, this benchmark loop is sufficient.

\subsection{Training}
For the training profiling we put our operations into \texttt{train} mode and omit the environment used in the inference case. In order to portray the training process for a single operation, we basically simply need to run a forward pass and a backward pass. The forward pass is handled identically. In order to execute the backward pass we need a substitute for the gradients flowing backward trough the model. This is achieved by preinitializing a random torch vector with the dimensions of our operation's output which we can call the backward pass on. Other than that we only have to make sure to have keep our gradients for the operation instances and the input vector reset and can proceed.


\section{Energy Profiling}

The energy profiling is conducted by measuring the power in watts which then can later be combined into energy results with the time measurements. \\
We are starting \texttt{nvidia-smi} as a background process and logging the power readings into a temporary csv file, which is evaluated later in the script to extract the statistics relevant to our purpose. On a higher level abstraction, a second instance of \texttt{nvidia-smi} runs during the complete benchmark process and can be compared to the timestamps included in the dataset of profiled operations, in order to investigate surprising annomalies in the results. The results relevant to our power profiling of the operations however, follow directly from the per operation temporary log file. In terms of direct measurement there is no additional complexity involved in this power profiling, however, some preparation and processing after the fact is necessary to ensure robust results. 
% 3 sigma cutoff
% warmup runs, also applying to timme measurement of course

\section{GPU Clocks}