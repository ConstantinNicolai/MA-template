\chapter{Dataset Collection}\label{chap:contrib1}

% This contribution serves to introduce the method used
% to collect a profiling dataset spanning a number of
% parameters. This serves as a training set for the
% prediction model. The dataset spans operations from
% Torchvision models and input sizes, execution time 
% and power, inference and training case as well as
% different GPUs and various GPU clocks.

This contribution outlines the method used to collect a profiling dataset. The  dataset serves as a training set for the prediction model. The parameters covered are various Torchvision models and input sizes, execution time and power, both the inference and the training case as well as different GPUs and various GPU clocks.


\section{Operations}


In order to increase generality our approach exploits the layerwise structure of DNNs. This is achieved by working on the layer level rather than on the model level. In order to be rigorous we need to be clear on the terminology. The workload and its characteristics depend on the layer and its input features, just like it would on the DNN and the input dimensions on the model level. We are interested in most general objects with an identical characteristic workload. On the layer level those will be layers with specific input feature dimensions and layer settings. We will refer to these objects as operations. On the model level those will be DNNs with specific input image dimensions, which we will refer to as model-input sets. \\
The units for which we will make predictions later are individual operations. To this end, we aim to collect a dataset of operations and profile the execution time and wattage for each one. To ensure that the dataset reflects operations encountered in real-world scenarios, we select a number of representative model-input sets from models of the Torchvision library. The set of operations which we will be profling, is the set of all operations occuring in any of these model-input sets. \\
Extracting this set is automated via scripts, but can just as well be done by hand if so desired. With this set at hand we can dive into the actual profiling.


\section{Time Profiling}

\input{Plots/runtime.tex}

The time profiling is performed using the benchmark utility from \texttt{torch.utils.benchmark} called Timer. This enables us to run our benchmark function with the specific layer and input features as input parameters, allowing us to run it for each operation. \\
To achieve more stable results and inprove the simultaneous power profiling it is desirable to run the benchmark for each operations for at least a few seconds. This is achieved by setting a minimum runtime for the benchmark to aggregate statistics. Different workload characteristics and different computational capabilities depending on the GPU configuration being profiled result in different requirements for individual profiling runs in order to ensure sufficient repitions in the measurement process. In order to accomodate this requirement, the runtime of the measurement is configurable via a command-line parameter. \\
For the initialization of the operations we have to be careful to avoid two extreme cases in order to achieve a sensible approximation of the execution characteristics within a neural network. The first case we want to avoid is initializing one instance of the operation and performing each benchmark loop on that instance. This would result in hitting the cash every time and would not be representative. The second extreme we want to avoid is initializing a new instance for each loop of the benchmark run, which adds the initializition overhead and the larger latency of having to access the GPU main memory every time, skewing our results in the opposite direction. We have therefore chosen the middleground approach of initializing a sizable block of operation instances with random weights and biases, which is then looped over by the benchmark kernel, resulting in some reuse of layers, stressing the memory system in a sensible manner. \\


\subsection{Inference}
The central difference between inference and training profiling appears in the design of the benchmark function. \\
For inference profiling we put our operations into \texttt{eval} mode and call the operations within a \texttt{torch.inference\_mode} environment, which is equivalent to the exection in a typical inference forward pass through a model. Within the loop we call the operator on a preinitialized random input tensor for the forward pass. The input tensor is a single instance, as it represents the activations from the previous layer which can be expected to live in high level memory in our memory hierarchy. Since this forward pass is everything we want to profile for inference, this benchmark loop is sufficient.

\subsection{Training}
For the training profiling we put our operations into \texttt{train} mode and omit the environment used in the inference case. In order to portray the training process for a single operation, we need to run a forward pass and a backward pass. The forward pass is handled identically. In order to execute the backward pass we need a substitute for the gradients flowing backward trough the model. This is achieved by preinitializing a random torch vector with the dimensions of our operation's output which we can then call the backward pass on. Other than that, we only have to make sure the runtime keeps the gradients for all operation instances and keep resetting the input vector gradients for each loop iteration.

\subsection{Proportionality}
Since we are assuming the number of benchmark iterations and the resulting total runtime to be proportional, we need to test that assumption. In order to do that we plot their relationship in Figure \ref{fig:proport}. By identifying how short the total runtime needs to become for the linear relationship to break, we can find a lower bound to our desired benchmark duration for each operation we study. The result of this investigation is that as long as we stay above 100 ms we are on the safe side. In our actual measurements every operation is benchmarked for at least several seconds, even though the exact duration may vary depending on the computational intensity of the specific operation.


\section{Energy Profiling}

% \input{Plots/startup.tex}

The energy profiling is conducted by measuring the power in watts. Combined with the time measurements this gives us the energy results. \\
We are starting \texttt{nvidia-smi} as a background process and logging the power readings into a temporary csv file, which is evaluated later in the script to extract the statistics relevant to our purpose. On a higher level abstraction, a second instance of \texttt{nvidia-smi} runs during the complete benchmark process. This log can be compared to the timestamps included in the dataset of profiled operations, in order to investigate surprising annomalies in the results. However, the results relevent to our power profiling are calculated instanly from operation specific temporary log file. In terms of direct measurement there is no additional complexity involved for the power profiling. Still, some preparation and processing after the fact is necessary to ensure robust results. \\
As can be seen in Figure ?, showing the power measurement over time for alternating idle times and benchmark calls, the transition is not instant. There are some power readings in between the steady states of idle and benchmark. \\
In order to illustrate the existance of a startup effect without idle times between the benchmark runs, Figure ? shows five looped benchmark runs of the same benchmark overlayed. For each run we can observe the startup effect. \\
We do not want to keep this startup effect in our result, because it is a result of our benchmark exection and not representative of the much more integrated execution pipeline in a neural network.\\
In order to keep it from affecting our results, we use a $3\sigma$ channel around the initial mean of the power readings and drop everything else. \\
As a second precaution, the script also employs some warmup runs in order to further mimimize the impact of the startup effect.\\


\section{Log file Evaluation}


The following section explains how we arrive at our results using the log files collected in the benchmark run. Below you can see a snippet from one of the log files. The fourth entry in each row is the power.

% \begin{small} % Change to \footnotesize or \scriptsize for even smaller text
\begin{verbatim}
2024/10/10 13:18:58.369, 81, 145.99, 4396, 11264
2024/10/10 13:18:58.407, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.428, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.439, 81, 182.11, 4396, 11264
2024/10/10 13:18:58.490, 81, 178.50, 4396, 11264
2024/10/10 13:18:58.514, 81, 178.50, 4396, 11264
2024/10/10 13:18:58.538, 81, 178.50, 4396, 11264
\end{verbatim}
% \end{small}

We begin with the power evaluation. The log file is read in via \texttt{pandas}\footnote{\href{https://pandas.pydata.org/}{pandas}}. Any existing rows containing a non-numerical value are dropped from the dataframe. We then find the standard deviation for the power and drop all rows containing a power reading outside a $3 \: \sigma $ range. The following formulae are used:

\begin{equation}
\overline{W} = \frac{1}{n} \sum_{i=1}^{n} W_i
\end{equation}

\begin{equation}
\sigma_W = \sqrt{\frac{1}{n - 1} \sum_{i=1}^{n} (W_i - \overline{W})^2}
\end{equation}

\begin{equation}
W_{filtered} = W \; \text{such that} \, |W - \overline{W}| < 3 \sigma_W
\end{equation}
    
With \( n \) being the number of timestamps and \( W \) being the power. \\
The same two formulae are used to find the mean power \( \overline{W}_{filtered} \) and standard deviation \( \sigma_{\overline{W}_{filtered}} \) of the filtered power. \\
Both the total runtime \(t_{tot}\) and its standard deviation \(\sigma_{t_{tot}} \) are provided by \texttt{torch.utils.benchmark}.
With power and time evaluated, we find the total run energy \( E_{tot} \) and its error \( \sigma_{E_{tot}} \).

\begin{equation}
E_{tot} = \overline{W}_{filtered} \cdot t_{tot}
\end{equation}

\begin{equation}
\sigma_{E_{tot}} = \sigma_{\overline{W}_{filtered}} \cdot t_{tot}
\end{equation}

From this, we find the time per iteration \( t \) and the energy per iteration \( e \), as well as the error for the time per iteration \( \sigma_t \) and for the energy per iteration \( \sigma_e \) with the number of iterations being \( N \).

\begin{equation}
t = \frac{t_{tot}}{N}
\end{equation}

\begin{equation}
e = \frac{E_{tot}}{N}
\end{equation}

\begin{equation}
\sigma_t = \frac{\sigma_{t_{tot}}}{N}
\end{equation}

\begin{equation}
\sigma_e = \frac{\sigma_{E_{tot}}}{N}
\end{equation}


% 3 sigma cutoff
% warmup runs, also applying to time measurement of course

\section{GPU Clocks}