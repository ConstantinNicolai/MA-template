
@inproceedings{wess_energy_2023,
	title = {Energy {Profiling} of {DNN} {Accelerators}},
	url = {https://ieeexplore.ieee.org/document/10456770/?arnumber=10456770},
	doi = {10.1109/DSD60849.2023.00018},
	abstract = {This paper introduces a novel methodology for assessing the energy efficiency of neural network accelerators at both layer and network granularity. The approach involves extracting per-layer timing reports from recorded power profiles. The power and energy consumption of three prominent neural network accelerators, namely the Intel Neural Compute Stick 2, the Coral Edge TPU, and the NXP i.MX8M Plus is evaluated for three different Deep Neural Networks (DNNs) using this method. The study investigates the relationship between decreasing sampling frequencies and the average error, as well as the detailed energy consumption of individual DNN layers and layer types. The findings reveal that latency outperforms the number of operations per layer as a predictor for both overall and dynamic energy, with errors of 10 \% and 100 \% respectively. The main conclusions are: a sampling frequency of 200 kHz is necessary to achieve an average error of 5 \%; the number of operations is an inadequate predictor of energy consumption; and specific hardware settings significantly influence power and energy consumption, emphasizing the need for their consideration in estimation.},
	urldate = {2024-11-27},
	booktitle = {2023 26th {Euromicro} {Conference} on {Digital} {System} {Design} ({DSD})},
	author = {Wess, Matthias and Dallinger, Dominik and Schnöll, Daniel and Bittner, Matthias and Götzinger, Maximilian and Jantsch, Axel},
	month = sep,
	year = {2023},
	note = {ISSN: 2771-2508},
	keywords = {Artificial neural networks, Deep Neural Networks, Energy consumption, Estimation, Hardware accelerators, Power analysis, Power demand, Power measurement, Throughput, Universal Serial Bus},
	pages = {53--60},
	file = {Full Text PDF:/home/c/Zotero/storage/H2TYWZV5/Wess et al. - 2023 - Energy Profiling of DNN Accelerators.pdf:application/pdf;IEEE Xplore Abstract Record:/home/c/Zotero/storage/G39WCDDJ/10456770.html:text/html},
}

@inproceedings{justus_predicting_2018,
	title = {Predicting the {Computational} {Cost} of {Deep} {Learning} {Models}},
	url = {https://ieeexplore.ieee.org/document/8622396/?arnumber=8622396},
	doi = {10.1109/BigData.2018.8622396},
	abstract = {Deep learning is rapidly becoming a go-to tool for many artificial intelligence problems due to its ability to outperform other approaches and even humans at many problems. Despite its popularity we are still unable to accurately predict the time it will take to train a deep learning network to solve a given problem. This training time can be seen as the product of the training time per epoch and the number of epochs which need to be performed to reach the desired level of accuracy. Some work has been carried out to predict the training time for an epoch - most have been based around the assumption that the training time is linearly related to the number of floating point operations required. However, this relationship is not true and becomes exacerbated in cases where other activities start to dominate the execution time. Such as the time to load data from memory or loss of performance due to non-optimal parallel execution. In this work we propose an alternative approach in which we train a deep learning network to predict the execution time for parts of a deep learning network. Timings for these individual parts can then be combined to provide a prediction for the whole execution time. This has advantages over linear approaches as it can model more complex scenarios. But, also, it has the ability to predict execution times for scenarios unseen in the training data. Therefore, our approach can be used not only to infer the execution time for a batch, or entire epoch, but it can also support making a well-informed choice for the appropriate hardware and model.},
	urldate = {2024-11-27},
	booktitle = {2018 {IEEE} {International} {Conference} on {Big} {Data} ({Big} {Data})},
	author = {Justus, Daniel and Brennan, John and Bonner, Stephen and McGough, Andrew Stephen},
	month = dec,
	year = {2018},
	keywords = {Benchmark, Computational modeling, Deep learning, Hardware, Machine Learning, Neural networks, Performance, Prediction, Predictive models, Timing, Training},
	pages = {3873--3882},
	file = {Full Text PDF:/home/c/Zotero/storage/23CHCK7Y/Justus et al. - 2018 - Predicting the Computational Cost of Deep Learning.pdf:application/pdf;IEEE Xplore Abstract Record:/home/c/Zotero/storage/3KFJAQKM/8622396.html:text/html},
}

@article{sk_powertrain_2024,
	title = {{PowerTrain}: {Fast}, generalizable time and power prediction models to optimize {DNN} training on accelerated edges},
	volume = {161},
	issn = {0167-739X},
	shorttitle = {{PowerTrain}},
	url = {https://www.sciencedirect.com/science/article/pii/S0167739X24003649},
	doi = {10.1016/j.future.2024.07.001},
	abstract = {Accelerated edge devices, like Nvidia’s Jetson with 1000+ CUDA cores, are increasingly used for DNN training and federated learning, rather than just for inferencing workloads. A unique feature of these compact devices is their fine-grained control over CPU, GPU, memory frequencies, and active CPU cores, which can limit their power envelope in a constrained setting while throttling the compute performance. Given this vast 10k+ parameter space, selecting a power mode for dynamically arriving training workloads to exploit power–performance trade-offs requires costly profiling for each new workload, or is done ad hoc. We propose PowerTrain, a transfer-learning approach to accurately predict the power and time that will be consumed when we train a given DNN workload (model + dataset) using any specified power mode (CPU/GPU/memory frequencies, core-count). It requires a one-time offline profiling of 1000s of power modes for a reference DNN workload on a single Jetson device (Orin AGX) to build Neural Network (NN) based prediction models for time and power. These NN models are subsequently transferred (retrained) for a new DNN workload, or even a different Jetson device, with minimal additional profiling of just 50 power modes to make accurate time and power predictions. These are then used to rapidly construct the Pareto front and select the optimal power mode for the new workload, e.g., to minimize training time while meeting a power limit. PowerTrain’s predictions are robust to new workloads, exhibiting a low MAPE of {\textless}6\% for power and {\textless}15\% for time on six new training workloads (MobileNet, YOLO, BERT, LSTM, etc.) for up to 4400 power modes, when transferred from a ResNet reference workload on Orin AGX. It is also resilient when transferred to two entirely new Jetson devices (Xavier AGX and Jetson Orin Nano) with prediction errors of {\textless}14.5\% and {\textless}11\%. These outperform baseline predictions by more than 10\% and baseline optimizations by up to 45\% on time and 88\% on power.},
	urldate = {2024-11-27},
	journal = {Future Generation Computer Systems},
	author = {S.k., Prashanthi and Taluri, Saisamarth and S, Beautlin and Karwa, Lakshya and Simmhan, Yogesh},
	month = dec,
	year = {2024},
	keywords = {DNN training, Edge accelerators, Edge computing, Performance modeling, Performance optimization},
	pages = {329--344},
	file = {ScienceDirect Snapshot:/home/c/Zotero/storage/8NDS6BCA/S0167739X24003649.html:text/html},
}

@article{wess_annette_2021,
	title = {{ANNETTE}: {Accurate} {Neural} {Network} {Execution} {Time} {Estimation} {With} {Stacked} {Models}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {{ANNETTE}},
	url = {https://ieeexplore.ieee.org/document/9306831/?arnumber=9306831},
	doi = {10.1109/ACCESS.2020.3047259},
	abstract = {With new accelerator hardware for Deep Neural Networks (DNNs), the computing power for Artificial Intelligence (AI) applications has increased rapidly. However, as DNN algorithms become more complex and optimized for specific applications, latency requirements remain challenging, and it is critical to find the optimal points in the design space. To decouple the architectural search from the target hardware, we propose a time estimation framework that allows for modeling the inference latency of DNNs on hardware accelerators based on mapping and layer-wise estimation models. The proposed methodology extracts a set of models from micro-kernel and multi-layer benchmarks and generates a stacked model for mapping and network execution time estimation. We compare estimation accuracy and fidelity of the generated mixed models, statistical models with the roofline model, and a refined roofline model for evaluation. We test the mixed models on the ZCU102 SoC board with Xilinx Deep Neural Network Development Kit (DNNDK) and Intel Neural Compute Stick 2 (NCS2) on a set of 12 state-of-the-art neural networks. It shows an average estimation error of 3.47\% for the DNNDK and 7.44\% for the NCS2, outperforming the statistical and analytical layer models for almost all selected networks. For a randomly selected subset of 34 networks of the NASBench dataset, the mixed model reaches fidelity of 0.988 in Spearman’s {\textbackslash}rho rank correlation coefficient metric.},
	urldate = {2024-11-27},
	journal = {IEEE Access},
	author = {Wess, Matthias and Ivanov, Matvey and Unger, Christoph and Nookala, Anvesh and Wendt, Alexander and Jantsch, Axel},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Estimation, Computational modeling, Hardware, Analytical models, Benchmark testing, Biological system modeling, Computer architecture, estimation, neural network hardware, Tools},
	pages = {3545--3556},
	file = {Full Text PDF:/home/c/Zotero/storage/F7IAV2EM/Wess et al. - 2021 - ANNETTE Accurate Neural Network Execution Time Es.pdf:application/pdf;IEEE Xplore Abstract Record:/home/c/Zotero/storage/XNU4RILI/9306831.html:text/html},
}

@misc{yu_runtime-based_2021,
	title = {A {Runtime}-{Based} {Computational} {Performance} {Predictor} for {Deep} {Neural} {Network} {Training}},
	url = {http://arxiv.org/abs/2102.00527},
	doi = {10.48550/arXiv.2102.00527},
	abstract = {Deep learning researchers and practitioners usually leverage GPUs to help train their deep neural networks (DNNs) faster. However, choosing which GPU to use is challenging both because (i) there are many options, and (ii) users grapple with competing concerns: maximizing compute performance while minimizing costs. In this work, we present a new practical technique to help users make informed and cost-efficient GPU selections: make performance predictions with the help of a GPU that the user already has. Our technique exploits the observation that, because DNN training consists of repetitive compute steps, predicting the execution time of a single iteration is usually enough to characterize the performance of an entire training process. We make predictions by scaling the execution time of each operation in a training iteration from one GPU to another using either (i) wave scaling, a technique based on a GPU's execution model, or (ii) pre-trained multilayer perceptrons. We implement our technique into a Python library called Habitat and find that it makes accurate iteration execution time predictions (with an average error of 11.8\%) on ResNet-50, Inception v3, the Transformer, GNMT, and DCGAN across six different GPU architectures. Habitat supports PyTorch, is easy to use, and is open source.},
	urldate = {2024-11-27},
	publisher = {arXiv},
	author = {Yu, Geoffrey X. and Gao, Yubo and Golikov, Pavel and Pekhimenko, Gennady},
	month = jun,
	year = {2021},
	note = {arXiv:2102.00527},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance},
	file = {Preprint PDF:/home/c/Zotero/storage/R5M5DARV/Yu et al. - 2021 - A Runtime-Based Computational Performance Predicto.pdf:application/pdf;Snapshot:/home/c/Zotero/storage/INJC7FPD/2102.html:text/html},
}

@inproceedings{li_path_2023,
	address = {New York, NY, USA},
	series = {{MICRO} '23},
	title = {Path {Forward} {Beyond} {Simulators}: {Fast} and {Accurate} {GPU} {Execution} {Time} {Prediction} for {DNN} {Workloads}},
	isbn = {9798400703294},
	shorttitle = {Path {Forward} {Beyond} {Simulators}},
	url = {https://dl.acm.org/doi/10.1145/3613424.3614277},
	doi = {10.1145/3613424.3614277},
	abstract = {Today, DNNs’ high computational complexity and sub-optimal device utilization present a major roadblock to democratizing DNNs. To reduce the execution time and improve device utilization, researchers have been proposing new system design solutions, which require performance models (especially GPU models) to help them with pre-product concept validation. Currently, researchers have been utilizing simulators to predict execution time, which provides high flexibility and acceptable accuracy, but at the cost of a long simulation time. Simulators are becoming increasingly impractical to model today’s large-scale systems and DNNs, urging us to find alternative lightweight solutions. To solve this problem, we propose using a data-driven method for modeling DNNs system performance. We first build a dataset that includes the execution time of numerous networks/layers/kernels. After identifying the relationships of directly known information (e.g., network structure, hardware theoretical computing capabilities), we discuss how to build a simple, yet accurate, performance model for DNNs execution time. Our observations on the dataset demonstrate prevalent linear relationships between the GPU kernel execution times, operation counts, and input/output parameters of DNNs layers. Guided by our observations, we develop a fast, linear-regression-based DNNs execution time predictor. Our evaluation using various image classification models suggests our method can predict new DNNs performance with a 7\% error and new GPU performance with a 15.2\% error. Our case studies also demonstrate how the performance model can facilitate future DNNs system research.},
	urldate = {2024-11-27},
	booktitle = {Proceedings of the 56th {Annual} {IEEE}/{ACM} {International} {Symposium} on {Microarchitecture}},
	publisher = {Association for Computing Machinery},
	author = {Li, Ying and Sun, Yifan and Jog, Adwait},
	month = dec,
	year = {2023},
	pages = {380--394},
	file = {Full Text PDF:/home/c/Zotero/storage/QBH2ZH56/Li et al. - 2023 - Path Forward Beyond Simulators Fast and Accurate .pdf:application/pdf},
}

@inproceedings{metz_fast_2023,
	title = {Fast and {Accurate}: {Machine} {Learning} {Techniques} for {Performance} {Estimation} of {CNNs} for {GPGPUs}},
	shorttitle = {Fast and {Accurate}},
	url = {https://ieeexplore.ieee.org/document/10196565/?arnumber=10196565},
	doi = {10.1109/IPDPSW59300.2023.00127},
	abstract = {High performance and on-time calculations of Machine Learning (ML) algorithms are essential for emerging technologies such as autonomous driving, Internet of Things (IoT) or edge computing. One of the major algorithms used in such systems is Convolutional Neural Networks (CNNs), which require high computational resources. That leads designers to leverage ML accelerators like GPGPUs to meet design constraints. However, selecting the most appropriate accelerator requires Design Space Exploration (DSE), which is usually time-consuming and needs high manual effort. In this paper, we present a novel automated approach, enabling designers to fast and accurately estimate the performance of CNNs for GPGPUs in the early stage of the design process. The proposed approach uses static analysis for feature extraction and Decision Tree regression analysis for the performance estimation model. Experimental results demonstrate that our approach can predict CNNs performance with an absolute percentage error of 5.73\% compared to the actual hardware.},
	urldate = {2024-11-27},
	booktitle = {2023 {IEEE} {International} {Parallel} and {Distributed} {Processing} {Symposium} {Workshops} ({IPDPSW})},
	author = {Metz, Christopher A. and Goli, Mehran and Drechsler, Rolf},
	month = may,
	year = {2023},
	keywords = {Estimation, Predictive models, Computer architecture, Convolutional Neural Networks, Edge Computing, Internet of Things, Machine learning, Machine learning algorithms, Performance Estimation, Static analysis, Topology},
	pages = {754--760},
	file = {Full Text PDF:/home/c/Zotero/storage/CVDTT4AG/Metz et al. - 2023 - Fast and Accurate Machine Learning Techniques for.pdf:application/pdf;IEEE Xplore Abstract Record:/home/c/Zotero/storage/CL86L7BK/10196565.html:text/html},
}

@article{sponner_ai-driven_2022,
	title = {{AI}-{Driven} {Performance} {Modeling} for {AI} {Inference} {Workloads}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	issn = {2079-9292},
	url = {https://www.mdpi.com/2079-9292/11/15/2316},
	doi = {10.3390/electronics11152316},
	abstract = {Deep Learning (DL) is moving towards deploying workloads not only in cloud datacenters, but also to the local devices. Although these are mostly limited to inference tasks, it still widens the range of possible target architectures significantly. Additionally, these new targets usually come with drastically reduced computation performance and memory sizes compared to the traditionally used architectures—and put the key optimization focus on the efficiency as they often depend on batteries. To help developers quickly estimate the performance of a neural network during its design phase, performance models could be used. However, these models are expensive to implement as they require in-depth knowledge about the hardware architecture and the used algorithms. Although AI-based solutions exist, these either require large datasets that are difficult to collect on the low-performance targets and/or limited to a small number of target platforms and metrics. Our solution exploits the block-based structure of neural networks, as well as the high similarity in the typically used layer configurations across neural networks, enabling the training of accurate models on significantly smaller datasets. In addition, our solution is not limited to a specific architecture or metric. We showcase the feasibility of the solution on a set of seven devices from four different hardware architectures, and with up to three performance metrics per target—including the power consumption and memory footprint. Our tests have shown that the solution achieved an error of less than 1 ms (2.6\%) in latency, 0.12 J (4\%) in energy consumption and 11 MiB (1.5\%) in memory allocation for the whole network inference prediction, while being up to five orders of magnitude faster than a benchmark.},
	language = {en},
	number = {15},
	urldate = {2024-11-27},
	journal = {Electronics},
	author = {Sponner, Max and Waschneck, Bernd and Kumar, Akash},
	month = jan,
	year = {2022},
	note = {Number: 15
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {machine learning, performance modeling, regression models},
	pages = {2316},
	file = {Full Text PDF:/home/c/Zotero/storage/S3JTIAJ9/Sponner et al. - 2022 - AI-Driven Performance Modeling for AI Inference Wo.pdf:application/pdf},
}

@inproceedings{metz_ml-based_2022,
	title = {{ML}-based {Power} {Estimation} of {Convolutional} {Neural} {Networks} on {GPGPUs}},
	url = {https://ieeexplore.ieee.org/document/9770153},
	doi = {10.1109/DDECS54261.2022.9770153},
	abstract = {The increasing application of Machine Learning (ML) techniques on the Internet of Things (IoTs) has led to the leverage of ML accelerators like General Purpose Computing on Graphics Processing Units (GPGPUs) in such devices. However, selecting the most appropriate accelerator for IoT devices is very challenging as they commonly have tight constraints e.g., low power consumption, latency, and cost of the final product. Hence, the design of such application-specific IoT devices becomes a time-consuming and effort-hungry process, that poses the need for accurate and effective automated assisting methods.In this paper, we present a novel approach to estimate the power consumption of CUDA-based Convolutional Neural Networks (CNNs) on GPGPUs in the early design phases. The proposed approach takes advantage of a hybrid technique where static analysis is used for features extraction and the K-Nearest Neighbor (K-NN) regression analysis is utilized for power estimation model generation. Using K-NN analysis, the power estimation model can even be created with small training datasets. Experimental results demonstrate that the proposed approach is able to predict CNNs power consumption up to a Absolute Percentage Error of 0.0003\% in comparison to the real hardware.},
	urldate = {2024-11-27},
	booktitle = {2022 25th {International} {Symposium} on {Design} and {Diagnostics} of {Electronic} {Circuits} and {Systems} ({DDECS})},
	author = {Metz, Christopher A. and Goli, Mehran and Drechsler, Rolf},
	month = apr,
	year = {2022},
	note = {ISSN: 2473-2117},
	keywords = {Estimation, Power demand, Training, Analytical models, Machine learning, Static analysis, Hybrid power systems},
	pages = {166--171},
	file = {Full Text PDF:/home/c/Zotero/storage/MPFULAE8/Metz et al. - 2022 - ML-based Power Estimation of Convolutional Neural .pdf:application/pdf;IEEE Xplore Abstract Record:/home/c/Zotero/storage/8P2CMWJY/9770153.html:text/html},
}

@article{lechner_blackthorn_2021,
	title = {Blackthorn: {Latency} {Estimation} {Framework} for {CNNs} on {Embedded} {Nvidia} {Platforms}},
	volume = {9},
	issn = {2169-3536},
	shorttitle = {Blackthorn},
	url = {https://ieeexplore.ieee.org/document/9503415},
	doi = {10.1109/ACCESS.2021.3101936},
	abstract = {With more powerful yet efficient embedded devices and accelerators being available for Deep Neural Networks (DNN), machine learning is becoming an integral part of edge computing. As the number of such devices increases, finding the best platform for a specific application has become more challenging. A common question for application developers is to find the most cost-effective combination of a DNN and a device while still meeting latency and accuracy requirements. In this work, we propose Blackthorn, a layer-wise latency estimation framework for embedded Nvidia GPUs based on analytical models. We provide accurate predictions for each layer, helping developers to find bottlenecks and optimize the architecture of a DNN to fit target platforms. Our framework can quickly evaluate and compare large amounts of network optimizations without needing to build time-consuming execution engines. Our experimental results on Jetson TX2 and Jetson Nano devices show a per-layer estimation error of 6.104\% Root-Mean-Square-Percentage-Error (RMSPE) and 5.888\% RMSPE, which significantly outperforms current state-of-the-art methods. At network level, the average latency error is below 3\% for the tested DNNs.},
	urldate = {2024-11-27},
	journal = {IEEE Access},
	author = {Lechner, Martin and Jantsch, Axel},
	year = {2021},
	note = {Conference Name: IEEE Access},
	keywords = {Artificial neural networks, Estimation, Hardware, Neural networks, Predictive models, Benchmark testing, estimation, neural network hardware, Tools, Graphics processing units},
	pages = {110074--110084},
	file = {Full Text PDF:/home/c/Zotero/storage/GDFHTU5C/Lechner and Jantsch - 2021 - Blackthorn Latency Estimation Framework for CNNs .pdf:application/pdf;IEEE Xplore Abstract Record:/home/c/Zotero/storage/4ASXURZD/9503415.html:text/html},
}

@misc{kaufman_learned_2021,
	title = {A {Learned} {Performance} {Model} for {Tensor} {Processing} {Units}},
	url = {http://arxiv.org/abs/2008.01040},
	doi = {10.48550/arXiv.2008.01040},
	abstract = {Accurate hardware performance models are critical to efﬁcient code generation. They can be used by compilers to make heuristic decisions, by superoptimizers as a minimization objective, or by autotuners to ﬁnd an optimal conﬁguration for a speciﬁc program. However, they are difﬁcult to develop because contemporary processors are complex, and the recent proliferation of deep learning accelerators has increased the development burden. We demonstrate a method of learning performance models from a corpus of tensor computation graph programs for Tensor Processing Unit (TPU) instances. We show that our learned model outperforms a heavily-optimized analytical performance model on two tasks—tile-size selection and operator fusion—and that it helps an autotuner discover faster programs in a setting where access to TPUs is limited or expensive.},
	language = {en},
	urldate = {2025-02-05},
	publisher = {arXiv},
	author = {Kaufman, Samuel J. and Phothilimthana, Phitchaya Mangpo and Zhou, Yanqi and Mendis, Charith and Roy, Sudip and Sabne, Amit and Burrows, Mike},
	month = mar,
	year = {2021},
	note = {arXiv:2008.01040 [cs]},
	keywords = {Computer Science - Machine Learning, Computer Science - Performance},
	annote = {Comment: A version will appear in the Proceedings of the 4th MLSys Conference, San Jose, CA, USA, 2021},
	file = {Kaufman et al. - 2021 - A Learned Performance Model for Tensor Processing .pdf:/home/c/Zotero/storage/RNQR9VUQ/Kaufman et al. - 2021 - A Learned Performance Model for Tensor Processing .pdf:application/pdf},
}

@article{qi_paleo_2017,
	title = {{PALEO}: {A} {PERFORMANCE} {MODEL} {FOR} {DEEP} {NEURAL} {NETWORKS}},
	abstract = {Although various scalable deep learning software packages have been proposed, it remains unclear how to best leverage parallel and distributed computing infrastructure to accelerate their training and deployment. Moreover, the effectiveness of existing parallel and distributed systems varies widely based on the neural network architecture and dataset under consideration. In order to efﬁciently explore the space of scalable deep learning systems and quickly diagnose their effectiveness for a given problem instance, we introduce an analytical performance model called PALEO. Our key observation is that a neural network architecture carries with it a declarative speciﬁcation of the computational requirements associated with its training and evaluation. By extracting these requirements from a given architecture and mapping them to a speciﬁc point within the design space of software, hardware and communication strategies, PALEO can efﬁciently and accurately model the expected scalability and performance of a putative deep learning system. We show that PALEO is robust to the choice of network architecture, hardware, software, communication schemes, and parallelization strategies. We further demonstrate its ability to accurately model various recently published scalability results for CNNs such as NiN, Inception and AlexNet.},
	language = {en},
	author = {Qi, Hang and Sparks, Evan R and Talwalkar, Ameet},
	year = {2017},
	file = {Qi et al. - 2017 - PALEO A PERFORMANCE MODEL FOR DEEP NEURAL NETWORK.pdf:/home/c/Zotero/storage/7456V5HM/Qi et al. - 2017 - PALEO A PERFORMANCE MODEL FOR DEEP NEURAL NETWORK.pdf:application/pdf},
}

@article{lattuada_performance_2022,
	title = {Performance prediction of deep learning applications training in {GPU} as a service systems},
	volume = {25},
	issn = {1386-7857, 1573-7543},
	url = {https://link.springer.com/10.1007/s10586-021-03428-8},
	doi = {10.1007/s10586-021-03428-8},
	abstract = {Data analysts predict that the GPU as a Service (GPUaaS) market will grow from US\$700 million in 2019 to \$7 billion in 2025 with a compound annual growth rate of over 38\% to support 3D models, animated video processing, and gaming. GPUaaS adoption will be also boosted by the use of graphics processing units (GPUs) to support Deep learning (DL) model training. Indeed, nowadays, the main cloud providers already oﬀer in their catalogs GPU-based virtual machines pre-installed with the popular DL framework (like Torch, PyTorch, TensorFlow, and Caﬀe) simplifying DL model programming operations.},
	language = {en},
	number = {2},
	urldate = {2025-02-05},
	journal = {Cluster Computing},
	author = {Lattuada, Marco and Gianniti, Eugenio and Ardagna, Danilo and Zhang, Li},
	month = apr,
	year = {2022},
	pages = {1279--1302},
	file = {Lattuada et al. - 2022 - Performance prediction of deep learning applicatio.pdf:/home/c/Zotero/storage/P9YPV6EP/Lattuada et al. - 2022 - Performance prediction of deep learning applicatio.pdf:application/pdf},
}

@inproceedings{wang_perfnet_2020,
	address = {Gwangju Republic of Korea},
	title = {{PerfNet}: {Platform}-{Aware} {Performance} {Modeling} for {Deep} {Neural} {Networks}},
	isbn = {978-1-4503-8025-6},
	shorttitle = {{PerfNet}},
	url = {https://dl.acm.org/doi/10.1145/3400286.3418245},
	doi = {10.1145/3400286.3418245},
	abstract = {The technology of deep learning has grown rapidly and been widely used in the industry. In addition to the accuracy of the deep learning (DL) models, system developers are also interested in comprehending their performance aspects to make sure that the hardware design and the systems deployed to meet the application demands. However, developing a performance model to serve the aforementioned purpose needs to take many issues into account, e.g. the DL model, the runtime software, and the system architecture, which is quite complex. In this work, we propose a multi-layer regression network, called PerfNet, to predict the performance of DL models on heterogeneous systems. To train the PerfNet, we develop a tool to collect the performance features and characteristics of DL models on a set of heterogeneous systems, including key hyper-parameters such as loss functions, network shapes, and dataset size, as well as the hardware specifications. Our experiments show that the results of our approach are more accurate than previously published methods. In the case of VGG16 on GTX1080Ti, PerfNet yields a mean absolute percentage error of 20\%, while the referenced work constantly overestimates with errors larger than 200\%.},
	language = {en},
	urldate = {2025-02-05},
	booktitle = {Proceedings of the {International} {Conference} on {Research} in {Adaptive} and {Convergent} {Systems}},
	publisher = {ACM},
	author = {Wang, Chuan-Chi and Liao, Ying-Chiao and Kao, Ming-Chang and Liang, Wen-Yew and Hung, Shih-Hao},
	month = oct,
	year = {2020},
	pages = {90--95},
	file = {Wang et al. - 2020 - PerfNet Platform-Aware Performance Modeling for D.pdf:/home/c/Zotero/storage/PS8LEUQN/Wang et al. - 2020 - PerfNet Platform-Aware Performance Modeling for D.pdf:application/pdf},
}

@article{gianniti_performance_nodate,
	title = {Performance {Prediction} of {GPU}-based {Deep} {Learning} {Applications}},
	abstract = {Recent years saw an increasing success in the application of deep learning methods across various domains and for tackling different problems, ranging from image recognition and classiﬁcation to text processing and speech recognition. In this paper we propose and validate an approach to model the execution time for training convolutional neural networks (CNNs) deployed on GPGPUs. We demonstrate that our approach is generally applicable to a variety of CNN models and different types of GPGPUs with high accuracy, aiming at the preliminary design phases for system sizing.},
	language = {en},
	author = {Gianniti, Eugenio and di Milano, Politecnico and Zhang, Li},
	file = {Gianniti et al. - Performance Prediction of GPU-based Deep Learning .pdf:/home/c/Zotero/storage/THLFZBFQ/Gianniti et al. - Performance Prediction of GPU-based Deep Learning .pdf:application/pdf},
}

@article{cai_neuralpower_nodate,
	title = {{NeuralPower} : {Predict} and {Deploy} {Energy}-{Eﬃcient} {Convolutional} {Neural} {Networks}},
	abstract = {How much energy is consumed for an inference made by a convolutional neural network (CNN)?” With the increased popularity of CNNs deployed on the wide-spectrum of platforms (from mobile devices to workstations), the answer to this question has drawn signiﬁcant attention. From lengthening battery life of mobile devices to reducing the energy bill of a datacenter, it is important to understand the energy eﬃciency of CNNs during serving for making an inference, before actually training the model. In this work, we propose NeuralPower : a layer-wise predictive framework based on sparse polynomial regression, for predicting the serving energy consumption of a CNN deployed on any GPU platform. Given the architecture of a CNN, NeuralPower provides an accurate prediction and breakdown for power and runtime across all layers in the whole network, helping machine learners quickly identify the power, runtime, or energy bottlenecks. We also propose the “energy-precision ratio” (EPR) metric to guide machine learners in selecting an energy-eﬃcient CNN architecture that better trades oﬀ the energy consumption and prediction accuracy. The experimental results show that the prediction accuracy of the proposed NeuralPower outperforms the best published model to date, yielding an improvement in accuracy of up to 68.5\%. We also assess the accuracy of predictions at the network level, by predicting the runtime, power, and energy of state-of-the-art CNN architectures, achieving an average accuracy of 88.24\% in runtime, 88.34\% in power, and 97.21\% in energy. We comprehensively corroborate the eﬀectiveness of NeuralPower as a powerful framework for machine learners by testing it on diﬀerent GPU platforms and Deep Learning software tools.},
	language = {en},
	author = {Cai, Ermao and Juan, Da-Cheng and Stamoulis, Dimitrios and Marculescu, Diana},
	file = {Cai et al. - NeuralPower  Predict and Deploy Energy-Eﬃcient Co.pdf:/home/c/Zotero/storage/HGTSG3FP/Cai et al. - NeuralPower  Predict and Deploy Energy-Eﬃcient Co.pdf:application/pdf},
}

@article{yeung_towards_nodate,
	title = {Towards {GPU} {Utilization} {Prediction} for {Cloud} {Deep} {Learning}},
	abstract = {Understanding the GPU utilization of Deep Learning (DL) workloads is important for enhancing resource-efﬁciency and cost-beneﬁt decision making for DL frameworks in the cloud. Current approaches to determine DL workload GPU utilization rely on online proﬁling within isolated GPU devices, and must be performed for every unique DL workload submission resulting in resource under-utilization and reduced service availability. In this paper, we propose a prediction engine to proactively determine the GPU utilization of heterogeneous DL workloads without the need for in-depth or isolated online proﬁling. We demonstrate that it is possible to predict DL workload GPU utilization via extracting information from its model computation graph. Our experiments show that the prediction engine achieves an RMSLE of 0.154, and can be exploited by DL schedulers to achieve up to 61.5\% improvement to GPU cluster utilization.},
	language = {en},
	author = {Yeung, Gingfung and Borowiec, Damian and Friday, Adrian and Harper, Richard and Garraghan, Peter},
	file = {Yeung et al. - Towards GPU Utilization Prediction for Cloud Deep .pdf:/home/c/Zotero/storage/QE8Y8277/Yeung et al. - Towards GPU Utilization Prediction for Cloud Deep .pdf:application/pdf},
}

@article{velasco-montero_previous_2020,
	title = {{PreVIous}: {A} {Methodology} for {Prediction} of {Visual} {Inference} {Performance} on {IoT} {Devices}},
	volume = {7},
	issn = {2327-4662},
	shorttitle = {{PreVIous}},
	url = {https://ieeexplore.ieee.org/document/9040398/?arnumber=9040398},
	doi = {10.1109/JIOT.2020.2981684},
	abstract = {This article presents PreVIous, a methodology to predict the performance of convolutional neural networks (CNNs) in terms of throughput and energy consumption on vision-enabled devices for the Internet of Things. CNNs typically constitute a massive computational load for such devices, which are characterized by scarce hardware resources to be shared among multiple concurrent tasks. Therefore, it is critical to select the optimal CNN architecture for a particular hardware platform according to the prescribed application requirements. However, the zoo of CNN models is already vast and rapidly growing. To facilitate a suitable selection, we introduce a prediction framework that allows to evaluate the performance of CNNs prior to their actual implementation. The proposed methodology is based on PreVIousNet, a neural network specifically designed to build accurate per-layer performance predictive models. PreVIousNet incorporates the most usual parameters found in state-of-the-art network architectures. The resulting predictive models for inference time and energy have been tested against comprehensive characterizations of seven well-known CNN models running on two different software frameworks and two different embedded platforms. To the best of our knowledge, this is the most extensive study in the literature concerning CNN performance prediction on low-power low-cost devices. The average deviation between predictions and real measurements is remarkably low, ranging from 3\% to 10\%. This means state-of-the-art modeling accuracy. As an additional asset, the fine-grained a priori analysis provided by PreVIous could also be exploited by neural architecture search (NAS) engines.},
	number = {10},
	urldate = {2025-02-06},
	journal = {IEEE Internet of Things Journal},
	author = {Velasco-Montero, Delia and Fernández-Berni, Jorge and Carmona-Galán, Ricardo and Rodríguez-Vázquez, Ángel},
	month = oct,
	year = {2020},
	note = {Conference Name: IEEE Internet of Things Journal},
	keywords = {Computational modeling, Hardware, Predictive models, Training, Benchmark testing, Computer architecture, Convolutional neural networks (CNNs), deep learning (DL), edge devices, inference performance, neural architecture search (NAS), Performance evaluation, vision-enabled Internet of Things (IoT)},
	pages = {9227--9240},
	file = {Full Text PDF:/home/c/Zotero/storage/YQ28GZQH/Velasco-Montero et al. - 2020 - PreVIous A Methodology for Prediction of Visual I.pdf:application/pdf;IEEE Xplore Abstract Record:/home/c/Zotero/storage/QTWNCHEW/9040398.html:text/html},
}

@book{hastie_elements_2009,
	address = {New York, NY},
	edition = {Second edition},
	series = {Springer series in statistics},
	title = {The elements of statistical learning: data mining, inference, and prediction},
	isbn = {978-0-387-84858-7},
	shorttitle = {The elements of statistical learning},
	url = {https://search.ebscohost.com/login.aspx?direct=true&scope=site&db=nlebk&db=nlabk&AN=277008},
	language = {eng},
	urldate = {2025-07-17},
	publisher = {Springer},
	author = {Hastie, Trevor and Tibshirani, Robert and Friedman, Jerome H.},
	year = {2009},
	keywords = {Statistik / Maschinelles Lernen},
	annote = {Ausgabebezeichnung auf der Startseite: "Second edition, corrected 7th printing" ; Includes bibliographical references and indexes},
}

@inproceedings{chen_xgboost_2016,
	address = {San Francisco California USA},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	shorttitle = {{XGBoost}},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2025-07-17},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	pages = {785--794},
	file = {Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:/home/c/Zotero/storage/53469MC3/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@inproceedings{chen_xgboost_2016-1,
	address = {San Francisco California USA},
	title = {{XGBoost}: {A} {Scalable} {Tree} {Boosting} {System}},
	copyright = {https://www.acm.org/publications/policies/copyright\_policy\#Background},
	shorttitle = {{XGBoost}},
	url = {https://dl.acm.org/doi/10.1145/2939672.2939785},
	doi = {10.1145/2939672.2939785},
	abstract = {Tree boosting is a highly eﬀective and widely used machine learning method. In this paper, we describe a scalable endto-end tree boosting system called XGBoost, which is used widely by data scientists to achieve state-of-the-art results on many machine learning challenges. We propose a novel sparsity-aware algorithm for sparse data and weighted quantile sketch for approximate tree learning. More importantly, we provide insights on cache access patterns, data compression and sharding to build a scalable tree boosting system. By combining these insights, XGBoost scales beyond billions of examples using far fewer resources than existing systems.},
	language = {en},
	urldate = {2025-07-18},
	booktitle = {Proceedings of the 22nd {ACM} {SIGKDD} {International} {Conference} on {Knowledge} {Discovery} and {Data} {Mining}},
	publisher = {ACM},
	author = {Chen, Tianqi and Guestrin, Carlos},
	month = aug,
	year = {2016},
	pages = {785--794},
	file = {Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:/home/c/Zotero/storage/LGCISEWX/Chen and Guestrin - 2016 - XGBoost A Scalable Tree Boosting System.pdf:application/pdf},
}

@misc{noauthor_r2_score_nodate,
	title = {r2\_score},
	url = {https://scikit-learn/stable/modules/generated/sklearn.metrics.r2_score.html},
	abstract = {Gallery examples: Effect of transforming the targets in regression model Failure of Machine Learning to infer causal effects L1-based models for Sparse Signals Non-negative least squares Ordinary L...},
	language = {en},
	urldate = {2025-07-18},
	journal = {scikit-learn},
	file = {Snapshot:/home/c/Zotero/storage/UVRRCY9V/sklearn.metrics.r2_score.html:text/html},
}

@book{hennessy2017computer,
 author = {Hennessy, John L. and Patterson, David A.},
 title = {Computer Architecture: A Quantitative Approach},
 year = {2017},
 edition = {5th},
 publisher = {Morgan Kaufmann Publishers Inc.},
 address = { Burlington, MA, USA}
}
